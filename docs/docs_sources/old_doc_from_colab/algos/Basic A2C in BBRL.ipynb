{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, we will implement a simple version of the A2C algorithm using BBRL. To understand this code, you need [to know more about BBRL](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1gSdkOBPkIQi_my9TtwJ-qWZQS0b2X7jt?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) and, most importantly, details about the [AutoResetGymAgent](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."]},{"cell_type":"markdown","metadata":{"id":"MD3A9UmNS2OX"},"source":["The A2C algorithm is explained in [this video](https://www.youtube.com/watch?v=BUmsTlIgrBI) and you can also read [the corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/a2c.pdf)."]},{"cell_type":"markdown","source":["WARNING: this implementation does not correspond to the A2C algorithm as presented in the paper. Instead of computing the advantage function from rollouts minus the value as baseline, it computes the advantage as an expectation over the temporal differences delta. By contrast, the advanced version in [this notebook](https://colab.research.google.com/drive/1C_mgKSTvFEF04qNc_Ljj0cZPucTJDFlO?usp=sharing) is correct, it computes the advantage using the Generalized Advantage Estimate (GAE)."],"metadata":{"id":"nlA9d7o9v6bU"}},{"cell_type":"markdown","metadata":{"id":"zJZDcDafp7Uf"},"source":["## Installation and Imports"]},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"code","source":["!pip install importlib-metadata==4.13.0"],"metadata":{"id":"yq6_0IftWquH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_a2c(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_a2c(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"j0MaggiOl4KU"},"outputs":[],"source":["import functools\n","import time\n","!pip install omegaconf\n","from omegaconf import OmegaConf\n","\n","import gym\n","!pip install git+https://github.com/osigaud/bbrl_gym.git\n","!pip install git+https://github.com/osigaud/bbrl.git\n","\n","import bbrl"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Imports"]},{"cell_type":"markdown","metadata":{"id":"caqhJYbe5YcO"},"source":["Below, we import standard python packages, pytorch packages and gym environments."]},{"cell_type":"markdown","metadata":{"id":"4l7sTVXbJBE_"},"source":["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import AutoResetGymAgent"]},{"cell_type":"markdown","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [A2C](http://proceedings.mlr.press/v48/mniha16.pdf) algorithm is an actor-critic algorithm. Thus we need an Actor agent, a Critic agent and an Environment agent. \n","The actor agent is built on an intermediate ProbAgent, see [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for explanations about the  ProbaAgent, the ActorAgent and the environment agent."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Xe2thODO7E40"},"outputs":[],"source":["class ProbAgent(Agent):\n","    def __init__(self, observation_size, hidden_size, n_actions):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, n_actions),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        scores = self.model(observation)\n","        probs = torch.softmax(scores, dim=-1)\n","        self.set((\"action_probs\", t), probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ARPW1Mmo7NB-"},"outputs":[],"source":["class ActorAgent(Agent):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, t, stochastic, **kwargs):\n","        probs = self.get((\"action_probs\", t))\n","        if stochastic:\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = probs.argmax(1)\n","\n","        self.set((\"action\", t), action)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","metadata":{"id":"Din6iU-1DnyH"},"source":["### CriticAgent"]},{"cell_type":"markdown","metadata":{"id":"Nf0mXQvbEw7V"},"source":["A CriticAgent is a one hidden layer neural network which takes an observation as input and whose output is the value of this observation. It thus implements a $V(s)$ function. It would be straightforward to define another CriticAgent (call it a CriticQAgent by contrast to a CriticVAgent) that would take an observation and an action as input."]},{"cell_type":"markdown","metadata":{"id":"bQLc3dywFiqy"},"source":[" The `squeeze(-1)` removes the last dimension of the tensor. TODO: explain why we need it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"g8y-63nq7Pjo"},"outputs":[],"source":["class CriticAgent(Agent):\n","    def __init__(self, observation_size, hidden_size):\n","        super().__init__()\n","        self.critic_model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        critic = self.critic_model(observation).squeeze(-1)\n","        self.set((\"critic\", t), critic)"]},{"cell_type":"markdown","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the A2C agent"]},{"cell_type":"markdown","metadata":{"id":"tJ0qhRVgVarb"},"source":["In this simple version of the A2C agent, we combine a ProbAgent and an Action agent, and wrap them with the environment into a TemporalAgent. By contrast, the critic agent is wrapped later on outside the function, as we also need access to the unwrapped agent."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"G8Uk_RQh8QrO"},"outputs":[],"source":["# Create the A2C Agent\n","def create_a2c_agent(cfg, env_agent):\n","  observation_size,  n_actions = env_agent.get_obs_and_actions_sizes()\n","  prob_agent = ProbAgent(\n","      observation_size, cfg.algorithm.architecture.actor_hidden_size, n_actions\n","  )\n","  action_agent = ActorAgent()\n","  critic_agent = CriticAgent(\n","    observation_size, cfg.algorithm.architecture.critic_hidden_size)\n","\n","  # Combine env and policy agents\n","  agent = Agents(env_agent, prob_agent, action_agent)\n","  # Get an agent that is executed on a complete workspace\n","  agent = TemporalAgent(agent)\n","  agent.seed(cfg.algorithm.seed)\n","  return agent, prob_agent, critic_agent"]},{"cell_type":"markdown","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","metadata":{"id":"E4BrXwTLdK0Z"},"source":["The logger class below is not generic, it is specifically designed in the context of this A2C colab."]},{"cell_type":"markdown","metadata":{"id":"VKYYp8IHLhd-"},"source":["The logger parameters are defined below in `params = { \"logger\":{ ...`"]},{"cell_type":"markdown","metadata":{"id":"rhwNN4oCNOhi"},"source":["In this colab, the logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation (see the parameters part below).\n","Note that the salina Logger is also saving the log in a readable format such that you can use `Logger.read_directories(...)` to read multiple logs, create a dataframe, and analyze many experiments afterward in a notebook for instance. "]},{"cell_type":"markdown","metadata":{"id":"10TUc-PHMqNm"},"source":["The code for the different kinds of loggers is available in the [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/bbrl/utils/logger.py) file."]},{"cell_type":"markdown","metadata":{"id":"c872tM4WM5FH"},"source":["Having logging provided under the hood is one of the features where using RL libraries like BBRL will allow you to save time."]},{"cell_type":"markdown","metadata":{"id":"lmsf5BENLz10"},"source":["`instantiate_class` is an inner BBRL mechanism. The `instantiate_class`function is available in the [`bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aOkauz_0H2GA"},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n"]},{"cell_type":"markdown","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setup the optimizer"]},{"cell_type":"markdown","metadata":{"id":"VzmEKF4J8qjg"},"source":["We use a single optimizer to tune the parameters of the actor (in the prob_agent part) and the critic (in the critic_agent part). It would be possible to have two optimizers which would work separately on the parameters of each component agent, but it would be more complicated because updating the actor requires the gradient of the critic."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YFfzXEu2WFWj"},"outputs":[],"source":["# Configure the optimizer over the a2c agent\n","def setup_optimizer(cfg, prob_agent, critic_agent):\n","  optimizer_args = get_arguments(cfg.optimizer)\n","  parameters = nn.Sequential(prob_agent, critic_agent).parameters()\n","  optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n","  return optimizer"]},{"cell_type":"markdown","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Execute agent"]},{"cell_type":"markdown","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [`__call__(...)`](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [a previous notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"t4wkNJqa_k5c"},"outputs":[],"source":["def execute_agent(cfg, epoch, workspace, agent):\n","  if epoch > 0:\n","      workspace.zero_grad()\n","      workspace.copy_n_last_steps(1)\n","      agent(\n","        workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True\n","      )\n","  else:\n","    agent(workspace, t=0, n_steps=cfg.algorithm.n_timesteps, stochastic=True)"]},{"cell_type":"markdown","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Compute critic loss"]},{"cell_type":"markdown","source":["In this basic version, the critic loss is computed by estimating the advantage as an expectation over the temporal difference error $\\delta$. This is not what the standard A2C algorithm does."],"metadata":{"id":"98HLbstkRcW0"}},{"cell_type":"markdown","metadata":{"id":"fxxobbxRaJXO"},"source":["Note the `critic[1:].detach()` in the computation of the temporal difference target. The idea is that we compute this target as a function of $V(s_{t+1})$, but we do not want to apply gradient descent on this $V(s_{t+1})$, we will only apply gradient descent to the $V(s_t)$ according to this target value."]},{"cell_type":"markdown","metadata":{"id":"Ngf5NHvWBbrE"},"source":["In practice, `x.detach()` detaches a computation graph from a tensor, so it avoids computing a gradient over this tensor."]},{"cell_type":"markdown","metadata":{"id":"fXwrjbueoDw6"},"source":["Note also the trick to deal with terminal states. If the state is terminal, $V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted into an int, it becomes a 1), we get the term. If `must_bootstrap` is False (=0), we are at a terminal state, so we ignore the term. This trick is used in many RL libraries, e.g. SB3."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2sepUK-gAM3u"},"outputs":[],"source":["def compute_critic_loss(cfg, reward, must_bootstrap, critic):\n","  # Compute temporal difference\n","  target = reward[:-1] + cfg.algorithm.discount_factor * critic[1:].detach() * must_bootstrap.int()\n","  delta = target - critic[:-1]\n","\n","  # Compute critic loss\n","  td_error = delta ** 2\n","  critic_loss = td_error.mean()\n","  return critic_loss, delta"]},{"cell_type":"markdown","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","metadata":{"id":"8ixFZeCRN6Y6"},"source":["Note that everything about the shared workspace between all the agents is completely hidden under the hood. This results in a gain of productivity, at the expense of having to dig into the BBRL code if you want to understand the details, change the multiprocessing model, etc."]},{"cell_type":"markdown","metadata":{"id":"gAnnEjF9L9gk"},"source":["This version uses an AutoResetGymAgent. If you haven't done so yet, read  [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing) which explains a lot of details. In particular, read it to understand the `execute_agents(...)` function, the `transition_workspace = train_workspace.get_transitions()` line. Read also [the notebook about TimeLimits](https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing) to know more about the computation of `must_bootstrap`."]},{"cell_type":"markdown","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. Several things need to be explained here.\n","- `optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n","- note that we sum all the losses, both for the critic and the actor, before applying back-propagation with `loss.backward()`. At first glance, summing these losses may look weird, as the actor and the critic receive different updates with different parts of the loss. This mechanism relies on the central property of tensor manipulation libraries like TensorFlow and pytorch. In pytorch, each loss tensor comes with its own graph of computation for back-propagating the gradient, in such a way that when you back-propagate the loss, the adequate part of the loss is applied to the adequate parameters.\n","These mechanisms are partly explained [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n","- since the optimizer has been set to work with both the actor and critic parameters, `optimizer.step()` will optimize both agents and pytorch ensure that each will receive its own part of the gradient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk85_sRWW-5s"},"outputs":[],"source":["def run_a2c(cfg):\n","  # 1)  Build the  logger\n","  logger = Logger(cfg)\n","  \n","  # 2) Create the environment agent\n","  env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","\n","  # 3) Create the A2C Agent\n","  a2c_agent, prob_agent, critic_agent = create_a2c_agent(cfg, env_agent)\n","\n","  # 4) Create the temporal critic agent to compute critic values over the workspace\n","  tcritic_agent = TemporalAgent(critic_agent)\n","\n","  # 5) Configure the workspace to the right dimension\n","  # Note that no parameter is needed to create the workspace. \n","  # In the training loop, calling the agent() and critic_agent() \n","  # will take the workspace as parameter\n","  train_workspace = Workspace()\n","\n","  # 6) Configure the optimizer over the a2c agent\n","  optimizer = setup_optimizer(cfg, prob_agent, critic_agent)\n","  \n","  # 7) Training loop\n","  epoch = 0\n","  for epoch in range(cfg.algorithm.max_epochs):\n","    # Execute the agent in the workspace\n","    execute_agent(cfg, epoch, train_workspace, a2c_agent)\n","\n","    # Compute the critic value over the whole workspace\n","    tcritic_agent(train_workspace, n_steps=cfg.algorithm.n_timesteps)\n","\n","    transition_workspace = train_workspace.get_transitions()\n","\n","    # Get relevant tensors (size are timestep x n_envs x ....)\n","\n","    critic, done, reward, action, action_probs, truncated = transition_workspace[\n","                \"critic\", \"env/done\", \"env/reward\", \"action\", \"action_probs\", \"env/truncated\"]\n","\n","    # Determines whether values of the critic should be propagated\n","    # True if the episode reached a time limit or if the task was not done\n","    # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n","    must_bootstrap = torch.logical_or(~done[1], truncated[1])\n","\n","    # Compute critic loss (see function above)\n","    critic_loss, delta = compute_delta_loss(cfg, reward, must_bootstrap, critic)\n","\n","    # Take the log probability of the actions performed, after some reorganization\n","    action_logp = action_probs[0].gather(1, action[0].view(-1, 1)).squeeze().log()\n","\n","    # Compute the policy gradient loss based on the log probability of the actions performed\n","    a2c_loss = action_logp * delta.detach()\n","    a2c_loss = a2c_loss.mean()\n","\n","    # Compute entropy loss\n","    entropy_loss = torch.distributions.Categorical(action_probs).entropy().mean()\n","\n","    # Store the losses for tensorboard display\n","    logger.log_losses(cfg, epoch, critic_loss, entropy_loss, a2c_loss)\n","\n","    # Compute the total loss\n","    loss = (\n","      -cfg.algorithm.entropy_coef * entropy_loss\n","      + cfg.algorithm.critic_coef * critic_loss\n","      - cfg.algorithm.a2c_coef * a2c_loss\n","    )\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","\n","    # Compute the cumulated reward on the final states\n","    creward = train_workspace[\"env/cumulated_reward\"]\n","    done = train_workspace[\"env/done\"]\n","    creward = creward[done]\n","    if creward.size()[0] > 0:\n","      # print(creward)\n","      logger.add_log(\"reward\", creward.mean(), epoch)"]},{"cell_type":"markdown","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JB2B8zELNWQd"},"outputs":[],"source":["params={\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 432,\n","    \"n_envs\": 2,\n","    \"n_timesteps\": 16,\n","    \"max_epochs\": 7000,\n","    \"discount_factor\": 0.95,\n","    \"entropy_coef\": 0.001,\n","    \"critic_coef\": 1.0,\n","    \"a2c_coef\": 0.1,\n","    \"architecture\":{\n","      \"actor_hidden_size\": 32,\n","      \"critic_hidden_size\": [24, 36],\n","    },\n","  },\n","\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","  \"optimizer\":\n","  {\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 0.01,\n","  }\n","}"]},{"cell_type":"markdown","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bT-yD1ZnnNBQ"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./tmp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l42OUoGROlSt"},"outputs":[],"source":["config=OmegaConf.create(params)\n","torch.manual_seed(config.algorithm.seed)\n","run_a2c(config)"]},{"cell_type":"markdown","metadata":{"id":"vx8c0n2dKuRS"},"source":["With the parameters provided in this colab, you should observe that the reward is collapsing after 6K time steps."]},{"cell_type":"markdown","metadata":{"id":"paHdoNlz9Lpg"},"source":["## What's next?"]},{"cell_type":"markdown","metadata":{"id":"F2OIv4em9Lpj"},"source":["The simple version of A2C above suffers from several limitations:\n","- During training, the cumulated reward is measured from the training agent itself while it is changing. It is a better practice to stop training and perform a few evaluations on the trained agent from time to time.\n","- separating the ProbAgent and the ActionAgent is nice for illustrating the properties of SaLinA, but it is not so convenient, for instance when one wants to know the action of the agent for any state without calling upon a workspace.\n","- The code above only illustrates A2C with discrete actions, though the algorithm can also deal with continuous actions. Doing so requires defining new Agent classes and uniformizing the way they are used to avoid using \"if discrete/continuous\" parts of codes."]},{"cell_type":"markdown","metadata":{"id":"RexnSCwFCIjO"},"source":["We will perform the improvements corresponding to removing all these limitations in [this notebook](https://colab.research.google.com/drive/1C_mgKSTvFEF04qNc_Ljj0cZPucTJDFlO?usp=sharing). We will also add a few features, such as saving and loading agents or drawing pictures of the policy and critic agents."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}