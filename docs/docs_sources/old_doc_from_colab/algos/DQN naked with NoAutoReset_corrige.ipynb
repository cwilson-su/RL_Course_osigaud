{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In this notebook, using BBRL, we code a simple version of the DQN algorithm without a replay buffer nor a target network so as to better understand the inner mechanisms. To understand this code, you need to know more about BBRL. You should first have a look at [the BBRL interaction model](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing), then [a first example](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)."]},{"cell_type":"markdown","source":["## Installation and Imports"],"metadata":{"id":"zJZDcDafp7Uf"}},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["### Installation"]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","metadata":{"id":"pDy9yuQH73tJ"},"source":["This is OmegaConf that makes it possible that by just defining the `def run_dqn(cfg):` function and then executing a long `params = {...}` variable at the bottom of this colab, the code is run with the parameters without calling an explicit main.\n","\n","More precisely, the code is run by calling\n","\n","`config=OmegaConf.create(params)`\n","\n","`run_dqn(config)`\n","\n","at the very bottom of the colab, after starting tensorboard."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0MaggiOl4KU","executionInfo":{"status":"ok","timestamp":1654845767931,"user_tz":-120,"elapsed":153808,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dc0c6362-eb9b-4cbc-e4a1-844fb2694d7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting omegaconf\n","  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 3.5 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 40.0 MB/s \n","\u001b[?25hCollecting PyYAML>=5.1.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 13.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=04121ac5e4e357196d702da69d85ff8ff651bc78620993e47a2b44f9f25bb636\n","  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: PyYAML, antlr4-python3-runtime, omegaconf\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.9.3 omegaconf-2.2.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/osigaud/my_gym.git\n","  Cloning https://github.com/osigaud/my_gym.git to /tmp/pip-req-build-cqr48y73\n","  Running command git clone -q https://github.com/osigaud/my_gym.git /tmp/pip-req-build-cqr48y73\n","Collecting mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git\n","  Cloning https://github.com/osigaud/SimpleMazeMDP.git to /tmp/pip-install-hjnf0pv3/mazemdp_16752283c31844baa70927bb2b86bb28\n","  Running command git clone -q https://github.com/osigaud/SimpleMazeMDP.git /tmp/pip-install-hjnf0pv3/mazemdp_16752283c31844baa70927bb2b86bb28\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from my-gym==0.0.1) (1.21.6)\n","Collecting gym==0.21.0\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 4.9 MB/s \n","\u001b[?25hCollecting Box2D\n","  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 38.1 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (3.2.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->my-gym==0.0.1) (1.3.0)\n","Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->my-gym==0.0.1) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21.0->my-gym==0.0.1) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21.0->my-gym==0.0.1) (4.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (1.4.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git->my-gym==0.0.1) (1.15.0)\n","Building wheels for collected packages: my-gym, mazemdp, gym\n","  Building wheel for my-gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for my-gym: filename=my_gym-0.0.1-py3-none-any.whl size=13929 sha256=c4eb66f90d2c0b1f357c8dce593dcd8123a4a13c5ec9be0ac83dca40331c10db\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ytxhk6kw/wheels/1c/6c/5c/ae7a2f747d7e4018a92c506849842cf048d83e7bf07fa2b7b5\n","  Building wheel for mazemdp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mazemdp: filename=mazemdp-0.0.0-py3-none-any.whl size=13712 sha256=1a7e69ea0717f339e2ab1e2af55e94e8593eb2ffa852426d33af28ff2bb8a7ec\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ytxhk6kw/wheels/af/fb/6d/924ef6fbd5b3be612a3bd67dde1534a9ff066da1153c149542\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616827 sha256=eaa7644794ec99ff11fe01357fb8213513f45a784a646d2df31dbe0db54139be\n","  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n","Successfully built my-gym mazemdp gym\n","Installing collected packages: mazemdp, gym, Box2D, my-gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","Successfully installed Box2D-2.3.10 gym-0.21.0 mazemdp-0.0.0 my-gym-0.0.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gym"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/osigaud/bbrl.git\n","  Cloning https://github.com/osigaud/bbrl.git to /tmp/pip-req-build-kk3vs10b\n","  Running command git clone -q https://github.com/osigaud/bbrl.git /tmp/pip-req-build-kk3vs10b\n","Collecting my_gym@ git+https://github.com/osigaud/my_gym.git\n","  Cloning https://github.com/osigaud/my_gym.git to /tmp/pip-install-wer0dl20/my-gym_d4a476767c3b4c98934772be79bb17e4\n","  Running command git clone -q https://github.com/osigaud/my_gym.git /tmp/pip-install-wer0dl20/my-gym_d4a476767c3b4c98934772be79bb17e4\n","Collecting protobuf==3.20.1\n","  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (0.12.0+cu113)\n","Requirement already satisfied: gym==0.21.0 in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (0.21.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (2.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (4.64.0)\n","Collecting hydra-core\n","  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (1.3.5)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (4.1.2.30)\n","Collecting xformers>=0.0.3\n","  Downloading xformers-0.0.11.tar.gz (219 kB)\n","\u001b[K     |████████████████████████████████| 219 kB 37.9 MB/s \n","\u001b[?25hRequirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (3.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from bbrl==0.0.1) (0.11.2)\n","Collecting mazemdp@ git+https://github.com/osigaud/SimpleMazeMDP.git\n","  Cloning https://github.com/osigaud/SimpleMazeMDP.git to /tmp/pip-install-wer0dl20/mazemdp_698db4bb7a3245d0939a06ed22d1e941\n","  Running command git clone -q https://github.com/osigaud/SimpleMazeMDP.git /tmp/pip-install-wer0dl20/mazemdp_698db4bb7a3245d0939a06ed22d1e941\n","Requirement already satisfied: Box2D in /usr/local/lib/python3.7/dist-packages (from my_gym@ git+https://github.com/osigaud/my_gym.git->bbrl==0.0.1) (2.3.10)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->bbrl==0.0.1) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0->bbrl==0.0.1) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0->bbrl==0.0.1) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0->bbrl==0.0.1) (3.8.0)\n","Collecting pyre-extensions==0.0.23\n","  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n","Collecting typing-inspect\n","  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.0.1) (4.9.3)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.0.1) (5.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core->bbrl==0.0.1) (21.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf->bbrl==0.0.1) (6.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.0.1) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.0.1) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.0.1) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bbrl==0.0.1) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->bbrl==0.0.1) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->bbrl==0.0.1) (2022.1)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->bbrl==0.0.1) (1.4.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (0.37.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (3.3.7)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (1.0.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (1.46.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->bbrl==0.0.1) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.0.1) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.0.1) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->bbrl==0.0.1) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bbrl==0.0.1) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->bbrl==0.0.1) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.0.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.0.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->bbrl==0.0.1) (2022.5.18.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bbrl==0.0.1) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->bbrl==0.0.1) (7.1.2)\n","Collecting mypy-extensions>=0.3.0\n","  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n","Building wheels for collected packages: bbrl, xformers\n","  Building wheel for bbrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bbrl: filename=bbrl-0.0.1-py3-none-any.whl size=39862 sha256=31a6812090d01ce0927be7753a5de6f662bd1343b528b1018218c4a60d7469ec\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-o9tc25yj/wheels/c3/8e/75/20c9b8e7a65c576c58e1a303940d54011354125e1248b3aeb2\n","  Building wheel for xformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for xformers: filename=xformers-0.0.11-cp37-cp37m-linux_x86_64.whl size=4962569 sha256=87673c6fb03b9769c2165bf2952fcfc9688cf9a052dd4b9773ea0b7fb6f5bd84\n","  Stored in directory: /root/.cache/pip/wheels/cd/68/bc/dfcdbce20dbf08da5f0ae62a75f270f20b2be0c8bb0f8c1587\n","Successfully built bbrl xformers\n","Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, protobuf, xformers, hydra-core, bbrl\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.2+zzzcolab20220527125636 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\n","Successfully installed bbrl-0.0.1 hydra-core-1.2.0 mypy-extensions-0.4.3 protobuf-3.20.1 pyre-extensions-0.0.23 typing-inspect-0.7.1 xformers-0.0.11\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}],"source":["import os\n","import functools\n","import time\n","!pip install omegaconf\n","from omegaconf import OmegaConf\n","\n","import gym\n","!pip install git+https://github.com/osigaud/my_gym.git\n","!pip install git+https://github.com/osigaud/bbrl.git\n","\n","import bbrl"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Imports"]},{"cell_type":"markdown","metadata":{"id":"caqhJYbe5YcO"},"source":["Below, we import standard python packages, pytorch packages and gym environments."]},{"cell_type":"markdown","metadata":{"id":"4l7sTVXbJBE_"},"source":["[OpenAI gym](https://gym.openai.com/) is a collection of benchmark environments to evaluate RL algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import gym"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.agents.agent import Agent\n","from bbrl import get_arguments, get_class, instantiate_class\n","\n","# The workspace is the main class in BBRL, this is where all data is collected and stored\n","from bbrl.workspace import Workspace\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, RemoteAgent, TemporalAgent\n","\n","# AutoResetGymAgent is an agent able to execute a batch of gym environments\n","# with auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gymb import NoAutoResetGymAgent\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger"]},{"cell_type":"markdown","metadata":{"id":"JVvAfhKm9S8p"},"source":["## Definition of agents"]},{"cell_type":"markdown","source":["\n","See [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) for previous explanations about agents and environment agents."],"metadata":{"id":"7_D4Fb4Fz6M1"}},{"cell_type":"markdown","source":["### The critic agent"],"metadata":{"id":"7KcLIBWwfRJL"}},{"cell_type":"markdown","metadata":{"id":"RdqzKSLKDtqz"},"source":["The [DQN](https://daiwk.github.io/assets/dqn.pdf) algorithm is a critic only algorithm. Thus we just need a Critic agent (which will also be used to output actions) and an Environment agent. We reuse the `DiscreteQAgent` class that we have already explained in [this notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing)."]},{"cell_type":"code","source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"],"metadata":{"id":"r_QIxxHNtBMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARPW1Mmo7NB-"},"outputs":[],"source":["class DiscreteQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        self.model = build_mlp(\n","            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n","        )\n","\n","    def forward(self, t, choose_action=True, **kwargs):\n","        obs = self.get((\"env/env_obs\", t))\n","        q_values = self.model(obs).squeeze(-1)\n","        self.set((\"q_values\", t), q_values)\n","        if choose_action:\n","            action = q_values.argmax(1)\n","            self.set((\"action\", t), action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","source":["### Creating an Exploration method"],"metadata":{"id":"yoG1eNBguNTN"}},{"cell_type":"markdown","source":["As Q-learning, DQN needs some exploration to prevent too early convergence. Here we will use the simple $\\epsilon$-greedy exploration method. The method is implemented as an agent which chooses an action based on the Q-values."],"metadata":{"id":"qvxOy0e180_6"}},{"cell_type":"code","source":["class EGreedyActionSelector(Agent):\n","    def __init__(self, epsilon):\n","        super().__init__()\n","        self.epsilon = epsilon\n","\n","    def forward(self, t, **kwargs):\n","        q_values = self.get((\"q_values\", t))\n","        nb_actions = q_values.size()[1]\n","        size = q_values.size()[0]\n","        is_random = torch.rand(size).lt(self.epsilon).float()\n","        random_action = torch.randint(low=0, high=nb_actions, size=(size,))\n","        max_action = q_values.max(1)[1]\n","        action = is_random * random_action + (1 - is_random) * max_action\n","        action = action.long()\n","        self.set((\"action\", t), action)"],"metadata":{"id":"Rk0DTVsLtz4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training and evaluation environments"],"metadata":{"id":"W0AgHYc2ywoS"}},{"cell_type":"markdown","source":["In actor-critic algorithms relying on a replay buffer, the actor can be trained at each step during an episode. Besides, the training signal is the reward obtained during these episodes. So it may seem natural to display a learning curve corresponding to the performance of the training agent along the set of training episodes."],"metadata":{"id":"G9TO_O8FN2BO"}},{"cell_type":"markdown","source":["But let us think of it. If the agent is changing during an episode, which agent are we truly evaluating? The one in the beginning of the episode? In the middle? In the end? We see that such evaluations based on an evolving agent makes no sense."],"metadata":{"id":"0ivQUOUcN6dk"}},{"cell_type":"markdown","source":["What makes more sense consists in training an agent for a number of steps, and then evaluating it on a few episode to determine the performance of that particular agent, then start again training. With this approach, the learning curve makes more sense, it shows the evolving performance of a succession of agents obtained after training sequences."],"metadata":{"id":"pEihZ6bEOB9H"}},{"cell_type":"markdown","source":["Separating training and evaluation provides additional opportunities. Often, we will train the agent using exploration, but we will evaluate it in a greedy, deterministic mode, as if the problem is truly an MDP, so deterministic policy can be optimal."],"metadata":{"id":"Pl8HRPtsODc2"}},{"cell_type":"markdown","source":["We build two environments: one for training and another one for evaluation. The same agent is connected to these two environments in two instances of TemporalAgent so that we train and evaluate the same network."],"metadata":{"id":"1gfbKZMFrghw"}},{"cell_type":"markdown","source":["In the context of this notebook, we will only use the [NoAutoResetGymAgent](https://github.com/osigaud/bbrl/blob/96e58f6e01065f6a551039c4b9f7c1036b5523e6/bbrl/agents/gyma.py#L331) class, which is explained in [this notebook](https://colab.research.google.com/drive/1EX5O03mmWFp9wCL_Gb_-p08JktfiL2l5?usp=sharing)."],"metadata":{"id":"ko5pHfa1ffNB"}},{"cell_type":"markdown","source":["In practice, it is more efficient to use an AutoResetGymAgent, as we do not want to waste time if the task is done in an environment sooner than in the others, but this is more involved so we keep this for [a later notebook](https://colab.research.google.com/drive/1H9_gkenmb_APnbygme1oEdhqMLSDc_bM?usp=sharing)."],"metadata":{"id":"jDM2Z0THyrtx"}},{"cell_type":"markdown","source":["By contrast, for evaluation, we just need to perform a fixed number of episodes (for statistics), thus it is more convenient to use a NoAutoResetGymAgent with a set of environments and just run one episode in each environment. Thus we can use the `env/done` stop variable and take the average over the cumulated reward of all environments."],"metadata":{"id":"7kN-SniayxRq"}},{"cell_type":"markdown","source":["To keep the story simple, we use a single environment for training."],"metadata":{"id":"U0PADT8xgvFm"}},{"cell_type":"code","source":["def get_env_agents(cfg):\n","    train_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        1,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","    get_class(cfg.gym_env),\n","    get_arguments(cfg.gym_env),\n","    cfg.algorithm.nb_evals,\n","    cfg.algorithm.seed,\n","    )\n","    return train_env_agent, eval_env_agent"],"metadata":{"id":"hT5mr2yGyeUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzxoIPtLVJ_i"},"source":["### Create the DQN agent"]},{"cell_type":"markdown","metadata":{"id":"aaNnZw3bXEYd"},"source":["Interestingly, the loop between the policy and the environment is first defined as a collection of agents, and then embedded into a single TemporalAgent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8Uk_RQh8QrO"},"outputs":[],"source":["def create_dqn_agent(cfg, train_env_agent, eval_env_agent):\n","    obs_size, act_size = train_env_agent.get_obs_and_actions_sizes()\n","    critic = DiscreteQAgent(obs_size, cfg.algorithm.architecture.hidden_size, act_size)\n","    explorer = EGreedyActionSelector(cfg.algorithm.epsilon)\n","    q_agent = TemporalAgent(critic)\n","    tr_agent = Agents(train_env_agent, critic, explorer)\n","    ev_agent = Agents(eval_env_agent, critic)\n","\n","    # Get an agent that is executed on a complete workspace\n","    train_agent = TemporalAgent(tr_agent)\n","    eval_agent = TemporalAgent(ev_agent)\n","    train_agent.seed(cfg.algorithm.seed)\n","    return train_agent, eval_agent, q_agent"]},{"cell_type":"markdown","metadata":{"id":"lU3cO6znHyDc"},"source":["### The Logger class"]},{"cell_type":"markdown","metadata":{"id":"E4BrXwTLdK0Z"},"source":["The logger class below is not generic, it is specifically designed in the context of this notebook."]},{"cell_type":"markdown","metadata":{"id":"VKYYp8IHLhd-"},"source":["The logger parameters are defined below in `params = { \"logger\":{ ...`"]},{"cell_type":"markdown","metadata":{"id":"rhwNN4oCNOhi"},"source":["In this notebook, the logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation (see the parameters part below).\n","Note that the BBRL Logger is also saving the log in a readable format such that you can use `Logger.read_directories(...)` to read multiple logs, create a dataframe, and analyze many experiments afterward in a notebook for instance. "]},{"cell_type":"markdown","metadata":{"id":"10TUc-PHMqNm"},"source":["The code for the different kinds of loggers is available in the [bbrl/utils/logger.py](https://github.com/osigaud/bbrl/blob/master/bbrl/utils/logger.py) file."]},{"cell_type":"markdown","metadata":{"id":"c872tM4WM5FH"},"source":["Having logging provided under the hood is one of the features where using RL libraries like BBRL will allow you to save time."]},{"cell_type":"markdown","metadata":{"id":"lmsf5BENLz10"},"source":["`instantiate_class` is an inner BBRL mechanism. The `instantiate_class`function is available in the [bbrl/__init__.py](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOkauz_0H2GA"},"outputs":[],"source":["class Logger():\n","\n","  def __init__(self, cfg):\n","    self.logger = instantiate_class(cfg.logger)\n","\n","  def add_log(self, log_string, loss, epoch):\n","    self.logger.add_scalar(log_string, loss.item(), epoch)\n","\n","  # Log losses\n","  def log_losses(self, cfg, epoch, critic_loss, entropy_loss, a2c_loss):\n","    self.add_log(\"critic_loss\", critic_loss, epoch)\n","    self.add_log(\"entropy_loss\", entropy_loss, epoch)\n","    self.add_log(\"a2c_loss\", a2c_loss, epoch)\n"]},{"cell_type":"markdown","source":["## Heart of the algorithm"],"metadata":{"id":"HIlzSRxN1Nmx"}},{"cell_type":"markdown","metadata":{"id":"YQNvhO_VAJbh"},"source":["### Computing the critic loss"]},{"cell_type":"markdown","source":["The role of the `compute_critic_loss` function is to implement the Bellman backup rule. In Q-learning, this rule was written:\n","\n","$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha [ r(s_t,a_t) + \\gamma \\max_a Q(s_{t+1},a) - Q(s_t,a_t)]$$\n","\n","In DQN, the update rule $Q \\leftarrow Q + \\alpha [\\delta] $ is replaced by a gradient descent step over the Q-network. \n","\n","We first compute a target value: $ target = r(s_t,a_t) + \\gamma \\max_a Q(s_{t+1},a)$ from a set of samples.\n","\n","Then we get a TD error $\\delta$ by substracting $Q(s_t,a_t)$ for these samples, \n","\n","and we use the squared TD error as a loss function: $ loss = (target - Q(s_t,a_t))^2$."],"metadata":{"id":"XDsjTsJrkmyP"}},{"cell_type":"markdown","source":["To implement the above calculation in BBRL, the difficulty consists in properly dealing with time indexes. We have left commented prints into the code so that you can have a look at the data structures during the computation."],"metadata":{"id":"bqLxDdMbiR-s"}},{"cell_type":"markdown","source":["The `compute_critic_loss` function receives rewards, q_values and actions as vectors (in practice, pytorch tensors) that have been computed over a complete episode."],"metadata":{"id":"vw-i-kxpixFh"}},{"cell_type":"markdown","source":["We need to take `reward[:-1]`, which means all the rewards but the last one, because in BBRL, GymAgents repeat the reward at the last time step, as explained in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."],"metadata":{"id":"tpJGiqxp39RU"}},{"cell_type":"markdown","source":["Conversely, to get $\\max_a Q(s_{t+1}, a)$, we need to ignore the first of the max_q values, using `max_q[1:]`."],"metadata":{"id":"7NRhhdJ-3ZTX"}},{"cell_type":"markdown","metadata":{"id":"fxxobbxRaJXO"},"source":["Note the `max_q[0].detach()` in the computation of the temporal difference target. First, the max_q[0] is because the max function returns both the max and the indexes of the max. Second, about the .detach(), the idea is that we compute this target as a function of $\\max_a Q(s_{t+1}, a)$, but we do not want to apply gradient descent on this $\\max_a Q(s_{t+1}, a)$, we will only apply gradient descent to the $Q(s_t, a_t)$ according to this target value. In practice, `x.detach()` detaches a computation graph from a tensor, so it avoids computing a gradient over this tensor."]},{"cell_type":"markdown","metadata":{"id":"fXwrjbueoDw6"},"source":["The `must_bootstrap` tensor is used as a trick to deal with terminal states. If the state is terminal, $Q(s_{t+1}, a)$ does not make sense. Thus we need to ignore this term. So we multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted into a float, it becomes a 1), we get the term. If `must_bootstrap` is False (=0), we are at a terminal state, so we ignore the term. This trick is used in many RL libraries, e.g. SB3. In [this notebook](https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj?usp=sharing) we explain how to compute `must_bootstrap` so as to properly deal with time limits. In this version we use full episodes, thus `must_bootstrap` will always be True for all steps but the last one."]},{"cell_type":"markdown","metadata":{"id":"Ngf5NHvWBbrE"},"source":["To compute $Q(s_t,a_t)$ we use the `torch.gather()` function. This function is a little tricky to use, see [this page](https://medium.com/analytics-vidhya/understanding-indexing-with-pytorch-gather-33717a84ebc4) for useful explanations.\n","\n","In particular, the q_vals output that we get is not properly conditioned, hence the need for the `qval[:-1]` (we ignore the last dimension)."]},{"cell_type":"markdown","source":["Finally we just need to compute the difference target - qvals, square it, take the mean and send it back as the loss."],"metadata":{"id":"bxMvPyPH6_yl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sepUK-gAM3u"},"outputs":[],"source":["# Compute the temporal difference loss from a dataset to update a critic\n","\n","def compute_critic_loss(cfg, reward, must_bootstrap, q_values, action):\n","    # print(q_values)\n","\n","    # We compute the max of Q-values over all actions\n","    max_q = q_values.max(-1)\n","    max_q = max_q[0].detach()\n","    # print(max_q)\n","    # print(\"r:\", reward)\n","\n","    # To get the max of Q(s_{t+1}, a), we take max_q[1:]\n","    # The same about must_bootstrap. \n","    target = (\n","        reward[:-1] + cfg.algorithm.discount_factor * max_q[1:] * must_bootstrap[1:].int()\n","    )\n","    # print(\"t:\", target, target.shape)\n","    # print(action, action.shape)\n","\n","    # To get Q(s,a), we use the torch.gather() function which needs a specific data preparation\n","    vals = q_values.squeeze()\n","    # print(\"v\", vals, vals.shape)\n","    qvals = torch.gather(vals, dim=1, index=action)\n","    qvals = qvals[:-1]\n","    # print(\"qvals\", qvals, qvals.shape)\n","    td = target - qvals\n","    # print(td, td.shape)\n","    # Compute critic loss\n","    td_error = td**2\n","    critic_loss = td_error.mean()\n","    # print(critic_loss)\n","    return critic_loss\n"]},{"cell_type":"markdown","metadata":{"id":"f2vq1OJHWCIE"},"source":["### Setting up the optimizer"]},{"cell_type":"markdown","metadata":{"id":"VzmEKF4J8qjg"},"source":["The optimizer is used to tune the parameters of the DQN agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFfzXEu2WFWj"},"outputs":[],"source":["# Configure the optimizer over the q agent\n","def setup_optimizer(cfg, q_agent):\n","    optimizer_args = get_arguments(cfg.optimizer)\n","    parameters = q_agent.parameters()\n","    optimizer = get_class(cfg.optimizer)(parameters, **optimizer_args)\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"Jmi91gANWT4z"},"source":["## Main training loop"]},{"cell_type":"markdown","metadata":{"id":"8ixFZeCRN6Y6"},"source":["Note that everything about the shared workspace between all the agents is completely hidden under the hood. This results in a gain of productivity, at the expense of having to dig into the BBRL code if you want to understand the details, change the multiprocessing model, etc."]},{"cell_type":"markdown","metadata":{"id":"I6SuPOdW_hxl"},"source":["### Agent execution"]},{"cell_type":"markdown","metadata":{"id":"WqlH-8DaVWx2"},"source":["This is the tricky part with BBRL, the one we need to understand in detail. The difficulty lies in the copy of the last step and the way to deal with the n_steps return."]},{"cell_type":"markdown","metadata":{"id":"bWAmm0pPotTC"},"source":["The call to `train_agent(workspace, t=1, n_steps=cfg.algorithm.n_timesteps - 1, stochastic=True)` makes the agent run a number of steps in the workspace. In practice, it calls the [__call__()](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py#L54) function which makes a forward pass of the agent network using the workspace data and updates the workspace accordingly."]},{"cell_type":"markdown","metadata":{"id":"Rn3MlNQ3qGPr"},"source":["Now, if we start at the first epoch (`epoch=0`), we start from the first step (`t=0`). But when subsequently we perform the next epochs (`epoch>0`), we must not forget to cover the transition at the border between the previous epoch and the current epoch. To avoid this risk, we copy the information from the last time step of the previous epoch into the first time step of the next epoch. This is explained in more details in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."]},{"cell_type":"markdown","metadata":{"id":"OFB1XFE5YEc6"},"source":["Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()` lines. \n","\n","`optimizer.zero_grad()` is necessary to cancel all the gradients computed at the previous iterations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk85_sRWW-5s"},"outputs":[],"source":["def run_dqn(cfg):\n","     # 1)  Build the  logger\n","    logger = Logger(cfg)\n","    best_reward = -10e9\n","\n","    # 2) Create the environment agent\n","    train_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        1,\n","        cfg.algorithm.seed,\n","    )\n","    eval_env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.nb_evals,\n","        cfg.algorithm.seed,\n","    )\n","\n","    # 3) Create the DQN-like Agent\n","    train_agent, eval_agent, q_agent = create_dqn_agent(\n","        cfg, train_env_agent, eval_env_agent\n","    )\n","\n","    # Note that no parameter is needed to create the workspace.\n","    # In the training loop, calling the train_agent\n","    # will take the workspace as parameter\n","\n","    # 6) Configure the optimizer\n","    optimizer = setup_optimizer(cfg, q_agent)\n","    nb_steps = 0\n","    tmp_steps = 0\n","    nb_measures = 0\n","\n","    while nb_measures < cfg.algorithm.nb_measures:\n","        train_workspace = Workspace()  # Used for training\n","        train_agent(train_workspace, t=0, stop_variable=\"env/done\", stochastic=True)\n","\n","        q_values, done, truncated, reward, action = train_workspace[\n","            \"q_values\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","        nb_steps += len(q_values)\n","        # Determines whether values of the critic should be propagated\n","        # True if the episode reached a time limit or if the task was not done\n","        # See https://colab.research.google.com/drive/1erLbRKvdkdDy0Zn1X_JhC01s1QAt4BBj\n","        must_bootstrap = torch.logical_or(~done, truncated)\n","        # Compute critic loss\n","        critic_loss = compute_critic_loss(cfg, reward, must_bootstrap, q_values, action)\n","\n","        # Store the loss for tensorboard display\n","        logger.add_log(\"critic_loss\", critic_loss, nb_steps)\n","\n","        optimizer.zero_grad()\n","        critic_loss.backward()\n","        torch.nn.utils.clip_grad_norm_(\n","            q_agent.parameters(), cfg.algorithm.max_grad_norm\n","        )\n","        optimizer.step()\n","\n","        if nb_steps - tmp_steps > cfg.algorithm.eval_interval:\n","            nb_measures += 1\n","            tmp_steps = nb_steps\n","            eval_workspace = Workspace()  # Used for evaluation\n","            eval_agent(\n","                eval_workspace, t=0, stop_variable=\"env/done\", choose_action=True\n","            )\n","            rewards = eval_workspace[\"env/cumulated_reward\"][-1]\n","            mean = rewards.mean()\n","            logger.add_log(\"reward\", mean, nb_steps)\n","            print(f\"nb_steps: {nb_steps}, reward: {mean}\")\n","            if cfg.save_best and mean > best_reward:\n","                best_reward = mean\n","                directory = \"./dqn_critic/\"\n","                if not os.path.exists(directory):\n","                    os.makedirs(directory)\n","                filename = directory + \"dqn_\" + str(mean.item()) + \".agt\"\n","                eval_agent.save_model(filename)\n","                "]},{"cell_type":"markdown","metadata":{"id":"uo6bc3zzKua_"},"source":["## Definition of the parameters"]},{"cell_type":"markdown","metadata":{"id":"36r4PAfvKx-f"},"source":["The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a tensorboard visualisation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JB2B8zELNWQd"},"outputs":[],"source":["params={\n","  \"save_best\": False,\n","  \"logger\":{\n","    \"classname\": \"bbrl.utils.logger.TFLogger\",\n","    \"log_dir\": \"./tmp/\" + str(time.time()),\n","    \"cache_size\": 10000,\n","    \"every_n_seconds\": 10,\n","    \"verbose\": False,    \n","    },\n","\n","  \"algorithm\":{\n","    \"seed\": 5,\n","    \"max_grad_norm\": 0.5,\n","    \"epsilon\": 0.02,\n","    \"n_envs\": 1,\n","    \"n_steps\": 100,\n","    \"eval_interval\": 500,\n","    \"nb_measures\": 200,\n","    \"nb_evals\": 10,\n","    \"discount_factor\": 0.99,\n","    \"architecture\":{\"hidden_size\": [256, 256]},\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","  \"optimizer\":\n","  {\n","    \"classname\": \"torch.optim.Adam\",\n","    \"lr\": 2e-3,\n","  }\n","}"]},{"cell_type":"markdown","metadata":{"id":"jp7jDeGkaoM1"},"source":["### Launching tensorboard to visualize the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l42OUoGROlSt"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./tmp\n","config=OmegaConf.create(params)\n","run_dqn(config)"]},{"cell_type":"markdown","source":["The version used in this colab uses $< s_t, a_t, r_t, s_{t+1}>$ samples. As an exercise, you may switch to $< s_t, a_t, r_{t+1}, s_{t+1}>$ samples, going back to the standard SaLinA notation. For that, replace the import to `bbrl.agents.gyma` instead of `gymb`, and change the temporal difference update rule (in `compute_critic_loss(...)`) accordingly. See [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5) for more explanations."],"metadata":{"id":"lou6LTvVK7YY"}},{"cell_type":"markdown","source":["## Exercise"],"metadata":{"id":"yzuP3r2kpZeT"}},{"cell_type":"markdown","source":["The goal of the exercise is to add a target network to get closer to the true DQN algorithm. For that, you need to realize the following steps:\n","- import copy, to make copies of the Q-network\n","- in the `create_dqn_agent` function, initialize a target_critic as a copy of the initial critic using `copy.deepcopy(...)`. Then build the corresponding target_q_agent. The function should return this agent in addition to the previous ones.\n","- in the `compute_critic_loss` function, add `target_q_values` in the parameters, and make so that the target value is computed based on these values rather than on Q-values.\n","- in the `run_dqn` function, after running the q_agent on the training workspace, run the target_q_agent on the same workspace. Then get the recorded q_values as target_q_values\n","- each the number of time steps has increased more than `cfg.algorithm.target_critic_update`, copy again the current q_agent into the target_q_agent, using `copy.deepcopy(...)`\n","- add `cfg.algorithm.target_critic_update` in the parameters, for instance every 5000 steps"],"metadata":{"id":"hluZhf4lpch8"}},{"cell_type":"markdown","source":["## What's next?"],"metadata":{"id":"paHdoNlz9Lpg"}},{"cell_type":"markdown","source":["To get a full DQN, we need to do the following:\n","- Add a replay buffer. We can add a replay buffer independently from the target network. The version with a replay buffer and no target network corresponds to [the NQF algorithm](https://link.springer.com/content/pdf/10.1007/11564096_32.pdf).\n","- Before adding the replay buffer, we will first move to a version of DQN which uses the AutoResetGymAgent. For that, you need to first read the content of [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5). Then you can move to [the notebook where we build a naked DQN using the AutoResetGymAgent](https://colab.research.google.com/drive/1H9_gkenmb_APnbygme1oEdhqMLSDc_bM).\n","- We should also add a few extra-mechanisms which are present in the full DQN version: starting to learn once the replay buffer is full enough, decreasing the exploration rate epsilon...\n","- We could also add visualization tools to visualize the learned Q network, by using the `plot_critic` function available in [bbrl.visu.plot_critics.py](https://github.com/osigaud/bbrl/blob/96e58f6e01065f6a551039c4b9f7c1036b5523e6/bbrl/visu/visu_critics.py#L13)\n"],"metadata":{"id":"F2OIv4em9Lpj"}},{"cell_type":"markdown","source":["We may also easily code DDQN, by just changing one line in the `compute_critic_loss` function"],"metadata":{"id":"IAy0LGPYUOcb"}}],"metadata":{"colab":{"collapsed_sections":[],"name":"DQN naked with NoAutoReset_corrige.ipynb","provenance":[{"file_id":"1raeuB6uUVUpl-4PLArtiAoGnXj0sGjSV","timestamp":1655024181850},{"file_id":"1H9_gkenmb_APnbygme1oEdhqMLSDc_bM","timestamp":1654845389029},{"file_id":"1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x","timestamp":1653892366111},{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}