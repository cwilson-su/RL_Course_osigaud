{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In [a previous notebook](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing), we have seen the BBRL data collection approach and illustrated it with a simple pair of random agents. In this colab we explain how to create RL agents based on neural networks using pytorch and how to create agents representing gym environments using the GymAgent classes from BBRL."]},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["## Installation and imports"]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","source":["Note that we install the `my_gym` library to make sure to import the gym version 0.21.0. The interface of the later versions has been modified and maybe incompatible with previous code."],"metadata":{"id":"aOxU5SuSKqFF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0MaggiOl4KU"},"outputs":[],"source":["import functools\n","import time\n","\n","!pip install omegaconf\n","import omegaconf\n","\n","import gym\n","!pip install git+https://github.com/osigaud/my_gym.git\n","\n","try:\n","  import bbrl\n","except ImportError:\n","  from IPython.display import clear_output \n","  !pip install git+https://github.com/osigaud/bbrl.git\n","  clear_output()\n","  import bbrl"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"markdown","source":["As in the previous notebook, we import BBRL agents"],"metadata":{"id":"PrIJUVV32l2f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.workspace import Workspace\n","\n","from bbrl.agents.agent import Agent\n","\n","from bbrl import get_class, get_arguments, instantiate_class\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent (e.g an Agent) over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, TemporalAgent\n","\n","# GymAgent (resp. AutoResetGymAgent) are agents able to execute a batch of gym environments\n","# without (resp. with) auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gyma import AutoResetGymAgent, NoAutoResetGymAgent\n","\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Other Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import my_gym"]},{"cell_type":"code","source":["from omegaconf import OmegaConf"],"metadata":{"id":"LvWedwMIJWOm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating Neural RL agents"],"metadata":{"id":"AtKPR25b288G"}},{"cell_type":"markdown","source":["We will build two types of agents:\n","- First, a stochastic actor that outputs a stochastic discrete action given a state. This agent will be made of two parts, one for generating probabilities over actions, and the other one for choosing an action according to these probabilities.\n","- Second, a deterministic critic that outputs Q-values for discrete actions given a state. Here we will introduce more general functions to build neural networks from a set of specified layer sizes. "],"metadata":{"id":"63m188vAJ5Bj"}},{"cell_type":"markdown","source":["### A probabilistic actor in two parts"],"metadata":{"id":"29InT1mQK4bl"}},{"cell_type":"markdown","source":["Here we replace the simple ActionAgent of the previous notebook with a combination of two agents: the probabilistic agent, which contains the neural network, and the action agent, which selects an action based on the probabilities resulting from the probabilistic agent."],"metadata":{"id":"PtFXRZRj3dGp"}},{"cell_type":"markdown","source":["#### Probabilistic Agent"],"metadata":{"id":"e3M35WE83ESs"}},{"cell_type":"markdown","source":["A ProbAgent is a one hidden layer neural network which takes an observation as input and whose output is a probability given by a final softmax layer."],"metadata":{"id":"zVhd6r8c4Ihe"}},{"cell_type":"markdown","source":["The first layers are built in the `__init__()` function using the simple `nn.Sequential(...)` model from pytorch. We will do something more sophisticated to deal with an arbitrary number of layers later in another notebook."],"metadata":{"id":"dDhvWRrQ6iHV"}},{"cell_type":"markdown","source":["Now, let us have a look at the `forward()` function, which is called each time the agent performs a step in the environment."],"metadata":{"id":"-URAC2Cz6-58"}},{"cell_type":"markdown","source":["To get the input observation from the environment we call\n","`observation = self.get((\"env/env_obs\", t))`\n","and that to perform an action in the environment we call\n","`self.set((\"action_probs\", t), probs)`. In between, we call `torch.softmax()` to get probabilities from the output layer of the network."],"metadata":{"id":"GTFr51h44Jod"}},{"cell_type":"code","source":["class ProbAgent(Agent):\n","    def __init__(self, observation_size, hidden_size, n_actions):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, n_actions),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        scores = self.model(observation)\n","        probs = torch.softmax(scores, dim=-1)\n","        self.set((\"action_probs\", t), probs)"],"metadata":{"id":"3aFquLLO3Zeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Actor Agent"],"metadata":{"id":"OlOJSlR83KCE"}},{"cell_type":"markdown","source":["The ActorAgent takes action probabilities as input (coming from the ProbAgent) and outputs an action. In the deterministic case it takes the argmax, in the stochastic case it samples from the Categorical distribution. This agent does not have a neural network, it just takes a decision from the output of the ProbAgent."],"metadata":{"id":"gRq0XvYA4-SA"}},{"cell_type":"code","source":["class ActorAgent(Agent):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, t, stochastic, **kwargs):\n","        probs = self.get((\"action_probs\", t))\n","        if stochastic:\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = probs.argmax(1)\n","\n","        self.set((\"action\", t), action)"],"metadata":{"id":"QgpXfN1e4_gs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that instead of having a ProbaAgent then an ActorAgent, we could have built a single agent containing both. We are doing this mainly to illustrate the capabilities of BBRL to combine agents."],"metadata":{"id":"OW0hRb605iH-"}},{"cell_type":"markdown","source":["Note also that this pair of agents is adequate for environment with discrete actions, but we need something different if the environment takes continuous actions. In that case, instead of a categorical distribution, we would rather use a Gaussian distribution. We will build other versions of these agents later in another notebook."],"metadata":{"id":"4lTRFolx5s9z"}},{"cell_type":"markdown","source":["### A deterministic critic agent"],"metadata":{"id":"84aMsnrELCoE"}},{"cell_type":"markdown","source":["The function below builds a multi-layer perceptron where the size of each layer is given in the `size` list. We also specify the activation function of neurons at each layer and optionally a different activation function for the final layer."],"metadata":{"id":"l_LkwKoEMPsX"}},{"cell_type":"code","source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"],"metadata":{"id":"kOmwets4LZub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `DiscreteQAgent` class implements a critic such the one used in DQN. It has one output neuron per action and its output is the Q-value of these actions given the state. "],"metadata":{"id":"6u8r5T5sMppN"}},{"cell_type":"markdown","source":["Note that as any BBRL agent, it has a forward function that takes a time state as input. This forward function outputs the Q-values at the corresponding time step. Additionally, if the critic is used to choose an action, it also outputs the chosen action at the same time step."],"metadata":{"id":"T_Rj1SZrNB6E"}},{"cell_type":"markdown","source":["Besides, it is also useful to get the network output (as a Q-value or as an action) given a state rather than a time step. This is what the `predict_action` and `predict_value` functions are used for."],"metadata":{"id":"BdNNM9B7Naz4"}},{"cell_type":"code","source":["class DiscreteQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        self.is_q_function = True\n","        self.model = build_mlp(\n","            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n","        )\n","\n","    def forward(self, t, choose_action=True, **kwargs):\n","        obs = self.get((\"env/env_obs\", t))\n","        q_values = self.model(obs).squeeze(-1)\n","        self.set((\"q_values\", t), q_values)\n","        if choose_action:\n","            action = q_values.argmax(1)\n","            self.set((\"action\", t), action)\n","\n","    def predict_action(self, obs, stochastic):\n","        q_values = self.model(obs).squeeze(-1)\n","        if stochastic:\n","            probs = torch.softmax(q_values, dim=-1)\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = q_values.argmax(0)\n","        return action\n","\n","    def predict_value(self, obs, action):\n","        q_values = self.model(obs).squeeze(-1)\n","        return q_values[0][action]"],"metadata":{"id":"O4u_vhwjLIfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01yjyr34OU4-"},"source":["## Creating the environment agent"]},{"cell_type":"markdown","source":["Now that we have an RL agent, let us create an environment. In the previous notebook, we were doing something simple building on the Agent class. Now we do something more sophisticated, building on the `GymAgent` class and encapsulating an OpenAI gym environment."],"metadata":{"id":"mk0wEvfC-97e"}},{"cell_type":"markdown","metadata":{"id":"BHaRqpSAGYDL"},"source":["### Using a gym environment"]},{"cell_type":"markdown","metadata":{"id":"O-7OSw9BGb7t"},"source":["The function below creates the environment. In OpenAI gym, an environment can be known by its name, here the string `env_name`. The environment is generally given a maximum number of steps, which is enforced by the `TimeLimit` wrapper. Therefore, we we do not need to add our own TimeLimit: this may break the episode termination behavior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","source":["To call the above function, we will use a reflexive instantiation mechanism and get the parameters of the function from the `params` dictionary, in the `\"env\":{\n","      \"classname\": \"__main__.make_env\",\n","      \"env_name\": \"CartPole-v1\",\n","    }` part."],"metadata":{"id":"QA7b5PigC7JQ"}},{"cell_type":"markdown","source":["Using this instantiation approach from a function is useful if you define a new env for instance i.e you just change the 'classname' and put the arguments of the constructor directly and everything will work fine. This may be not natural a first sight, but if you start to use it, you will never go back again :) "],"metadata":{"id":"E448hsHtDcX6"}},{"cell_type":"markdown","source":["The `instantiate_class`, `get_class` and `get_arguments` functions are available in the [`main/bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file. The `get_class` function reads the `classname` in the parameters to create the appropriate type of object, and the `get_arguments` function reads the local paremeters and their values to set them into the corresponding object. "],"metadata":{"id":"VFmPRnp4DlUH"}},{"cell_type":"markdown","source":["### Creating the environment agent"],"metadata":{"id":"TYxjcMnvvl7d"}},{"cell_type":"markdown","source":["Now, let us create the agent representing the environment. We do so using the `NoAutoResetGymAgent` which inherits from the [GymAgent](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/gyma.py#L76) class and is provided by BBRL.\n","Essential information about this class is given in [this notebook](https://colab.research.google.com/drive/1EX5O03mmWFp9wCL_Gb_-p08JktfiL2l5?usp=sharing)."],"metadata":{"id":"RHnrZAScB_f8"}},{"cell_type":"markdown","source":["In the `get_env(cfg)` function below, the `NoAutoResetGymAgent` is created taking as arguments the environment creation function (here the `make_env` function that we defined above) with its parameters, then the number of environments and a seed. This seed serves to initialize the random number generator so that using the same seed will generate the same numbers again."],"metadata":{"id":"1KBSOl5lx-Pp"}},{"cell_type":"markdown","source":["These parameters are specified into a specific dictionary called `config` which is described below."],"metadata":{"id":"Pgs0g2Z7yoNA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODluTUN5L7jn"},"outputs":[],"source":["def get_env(cfg):\n","    env_agent = NoAutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    return env_agent"]},{"cell_type":"markdown","source":["## Running the agents along a single NoAutoReset episode"],"metadata":{"id":"yeDCeL91-ndI"}},{"cell_type":"markdown","source":["Before running everything, we need to get the parameters of the algorithm and the environment agent. To do this, we use the `omegaconf` package which builds a dictionary from a configuration string."],"metadata":{"id":"CRir-41GI6io"}},{"cell_type":"code","source":["params={\n","  \"algorithm\":{\n","    \"seed\": 432,\n","    \"n_envs\": 1,\n","    \"architecture\":{\"hidden_size\": 32},\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","}"],"metadata":{"id":"uFPvA57oGmNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As in [this notebook](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing), once we have the RL agent and the environment agent, we bind them together into a TemporalAgent"],"metadata":{"id":"xRHc2XD_u9su"}},{"cell_type":"markdown","source":["We have to create the environnement agent first, because we need to know the size of the observation and action spaces to create the action agent which will interact with it."],"metadata":{"id":"ghfUbhSyxOVe"}},{"cell_type":"code","source":["config = OmegaConf.create(params)\n","\n","env_agent = get_env(config)\n","observation_size, n_actions = env_agent.get_obs_and_actions_sizes()\n","prob_agent = ProbAgent(observation_size, config.algorithm.architecture.hidden_size, n_actions)\n","action_agent = ActorAgent()\n","composed_agent = Agents(env_agent, prob_agent, action_agent)\n","  \n","# Get a temporal agent that can be executed in a workspace\n","t_agent = TemporalAgent(composed_agent)"],"metadata":{"id":"nDteizkPvEzz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And finally we execute it in the workspace. Here, we run for 30 steps."],"metadata":{"id":"NYFD4-kGviQC"}},{"cell_type":"code","source":["# We create a workspace\n","workspace = Workspace()\n","\n","# The temporal agent will be run for 10 steps on this workspace\n","t_agent(workspace, t=0, n_steps=30, stochastic=True)\n","\n","# We retrieve the information as they are stored into the workspace\n","obs, action, reward, done = workspace[\"env/env_obs\", \"action\", \"env/reward\", \"env/done\"]\n","\n","# And we print them\n","print(\"obs:\", obs)\n","print(\"action:\", action)\n","print(\"reward:\", reward)\n","print(\"done:\", done)\n","# You should see that each variable has been recorded for the number of specified \n","# time steps..."],"metadata":{"id":"0uDlS_bMvmmp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you run the above interaction loop for enough steps (say 30) and you look closely at the result on the above cell, you will see that after the task is done (that is, the pole falls down in our case), the workspace continues filling data with copies of the last time step.\n","\n","We explain in details in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)  why this is so, together with several issues about data collection."],"metadata":{"id":"F2OIv4em9Lpj"}},{"cell_type":"markdown","source":["### Exercise"],"metadata":{"id":"uOHWGXAOZEIQ"}},{"cell_type":"markdown","source":["Just do the same using a DiscreteQAgent. Print the Q-values stored into the workspace."],"metadata":{"id":"qmu6S60KZGT3"}},{"cell_type":"code","source":["# Your code here"],"metadata":{"id":"fJSwYy9dZKjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## What's next?"],"metadata":{"id":"paHdoNlz9Lpg"}},{"cell_type":"markdown","source":["We are now ready to write a first version of the DQN algorithm, using the `DiscreteQAgent` defined above. We do so in [this notebook](https://colab.research.google.com/drive/1H9_gkenmb_APnbygme1oEdhqMLSDc_bM?usp=sharing)\n"],"metadata":{"id":"-AyAh795CJfc"}},{"cell_type":"markdown","source":["Or we can switch directly to using the AutoResetGymAgent class. We do so in [this notebook](https://colab.research.google.com/drive/1VJUoDGhxKv3mmFjTmLj_JDpappVw29xh?usp=sharing)"],"metadata":{"id":"X3_nZLp8wfjQ"}}],"metadata":{"colab":{"collapsed_sections":[],"name":"Building RL agents in NoAutoReset_corrige.ipynb","provenance":[{"file_id":"1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh","timestamp":1655024240391},{"file_id":"1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x","timestamp":1650560438416},{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}