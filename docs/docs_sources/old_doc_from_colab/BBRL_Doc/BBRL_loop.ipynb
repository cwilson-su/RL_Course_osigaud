{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p","timestamp":1660899946227},{"file_id":"1sBZLs-GaM8Xx7MsF6sUH7LIj6GwCq5VW","timestamp":1649224102197},{"file_id":"https://github.com/araffin/rl-handson-rlvs21/blob/main/rlvs_hands_on_sb3.ipynb","timestamp":1637247722561}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# BBRL in practice: the interaction loop"],"metadata":{"id":"QEZrc4hX3Lq8"}},{"cell_type":"markdown","source":["## Outlook"],"metadata":{"id":"QcPH8heX3Tfo"}},{"cell_type":"markdown","metadata":{"id":"hyyN-2qyK_T2"},"source":["In this notebook, we start practicing with the BBRL model, which is explained in [this notebook](https://colab.research.google.com/drive/1_yp-JKkxh_P8Yhctulqm0IrLbE41oK1p?usp=sharing). We just implement a simple interaction loop.\n"]},{"cell_type":"markdown","source":["What you will see here is very close to what Ludovic Denoyer shows in [this video](https://www.youtube.com/watch?v=CSkkoq_k5zU)."],"metadata":{"id":"awuajxx7femU"}},{"cell_type":"markdown","source":["# Installation"],"metadata":{"id":"MwRp50w1crBw"}},{"cell_type":"markdown","source":["Just run the following cell."],"metadata":{"id":"MIPSHXzucv9z"}},{"cell_type":"markdown","source":["Note the trick: we first try to import, if it fails we install the github repository and import again."],"metadata":{"id":"pmqxcBjCdBGr"}},{"cell_type":"code","metadata":{"id":"gWskDE2c9WoN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685512842713,"user_tz":-120,"elapsed":39917,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}},"outputId":"1f1d0a4c-973c-49a3-ec1e-8fd3e708dceb"},"source":["try:\n","  import bbrl\n","except ImportError:\n","  !pip install git+https://github.com/osigaud/bbrl.git\n","  import bbrl"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/osigaud/bbrl.git\n","  Cloning https://github.com/osigaud/bbrl.git to /tmp/pip-req-build-mbzhenyd\n","  Running command git clone --filter=blob:none --quiet https://github.com/osigaud/bbrl.git /tmp/pip-req-build-mbzhenyd\n","  Resolved https://github.com/osigaud/bbrl.git to commit 4d19640b3c9fc794ff5f65b55675f1001d6a1742\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: bbrl\n","  Building wheel for bbrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bbrl: filename=bbrl-0.1.11-py3-none-any.whl size=57146 sha256=4e83ac606b32cdddd835e2f4aaf7b48ae3589a284c503a0cb0c292245129595e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wo83a10d/wheels/67/8e/1c/cce356c5d6dc282da1f8fa1c1b34fcbee1bf22f260cc940d6c\n","Successfully built bbrl\n","Installing collected packages: bbrl\n","Successfully installed bbrl-0.1.11\n"]}]},{"cell_type":"code","source":["import torch # just used to get a random Tensor\n"],"metadata":{"id":"jvsrkHIa7wYX","executionInfo":{"status":"ok","timestamp":1685512842718,"user_tz":-120,"elapsed":11,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## BBRL imports"],"metadata":{"id":"5LzpfNl37eTp"}},{"cell_type":"markdown","source":["As explained in [the white paper](https://arxiv.org/pdf/2110.07910.pdf), everything in SaLinA (and also in BBRL) is an Agent."],"metadata":{"id":"Ql4Py3sz7W2Y"}},{"cell_type":"markdown","source":["This construct is defined in [the bbrl/agents/agent.py](https://github.com/osigaud/bbrl/blob/master/bbrl/agents/agent.py) file as the Agent class."],"metadata":{"id":"Z2vcM-la7LVp"}},{"cell_type":"markdown","source":["Any Agent class should come with a `forward(self, t, **kwargs)` method where t represents a time step."],"metadata":{"id":"mv02T_Yg05xO"}},{"cell_type":"markdown","source":["Some of the comments below are just copy-pasted from the paper or from the code."],"metadata":{"id":"X7PycXB274ZP"}},{"cell_type":"code","source":["from bbrl.workspace import Workspace\n","\n","from bbrl.agents.agent import Agent\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent (e.g an Agent) over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, TemporalAgent\n","\n","# GymAgent (resp. AutoResetGymAgent) are agents able to execute a batch of gym environments\n","# without (resp. with) auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gyma import AutoResetGymAgent, NoAutoResetGymAgent"],"metadata":{"id":"fn-61mNmm1uF","executionInfo":{"status":"ok","timestamp":1685512850237,"user_tz":-120,"elapsed":1211,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Remember that a workspace contains tensors, so everything written into a workspace should be a tensor. In the examples below the agents will first write random tensors."],"metadata":{"id":"0VYx-5X55Ppl"}},{"cell_type":"markdown","source":["# Creating and running agents"],"metadata":{"id":"RW8PjBXzYdRQ"}},{"cell_type":"markdown","source":["To play with the BBRL model, we first create a simple ActionAgent"],"metadata":{"id":"e9-uF4ySu2ke"}},{"cell_type":"code","source":["class ActionAgent(Agent):\n","    # Create the action agent\n","    # This is a fake agent for illustration purpose\n","    # In a standard ActionAgent, there should be an architecture \n","    # to compute the action given the observation\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, t, **kwargs):\n","        obs = self.get((\"obs\", t))\n","        action = torch.rand(1) # here should be function of the obs \n","\n","        self.set((\"action\", t), action)"],"metadata":{"id":"ZKswqGnrsXAb","executionInfo":{"status":"ok","timestamp":1685512854585,"user_tz":-120,"elapsed":776,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Then we create an EnvAgent"],"metadata":{"id":"-Zw2bN0su6Uu"}},{"cell_type":"code","source":["class EnvAgent(Agent):\n","  # Create the environment agent\n","  # This is a fake agent for illustration purpose\n","  # A standard EnvAgent would inherit from a GymAgent \n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, t, **kwargs):\n","    if t==0:\n","      # If we are in the first step, the agent has not acted yet\n","      # A real GymAgent would call obs = reset()\n","      obs = torch.rand(2)      \n","      reward = torch.randint(low=0, high=5, size=[1])     \n","      done = torch.zeros(1, dtype=torch.bool)\n","    else:\n","      # Here, a real GymAgent would call obs, reward, done, info = step(action)\n","      action = self.get((\"action\", t-1)) # beware, we take the previous action\n","      obs = torch.rand(2)           \n","      reward = torch.randint(low=0, high=5, size=[1])       \n","      done = torch.zeros(1, dtype=torch.bool)\n","    self.set((\"obs\", t), obs)\n","    self.set((\"reward\", t), reward)\n","    self.set((\"done\", t), done)\n"],"metadata":{"id":"Ze-bma4DtMQ_","executionInfo":{"status":"ok","timestamp":1685512857507,"user_tz":-120,"elapsed":321,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["We bind them together into a TemporalAgent"],"metadata":{"id":"xRHc2XD_u9su"}},{"cell_type":"code","source":["action_agent = ActionAgent()\n","env_agent = EnvAgent()\n","\n","# Compose both previous agents\n","composed_agent = Agents(env_agent, action_agent)\n","  \n","# Get a temporal agent that can be executed in a workspace\n","t_agent = TemporalAgent(composed_agent)"],"metadata":{"id":"nDteizkPvEzz","executionInfo":{"status":"ok","timestamp":1685512860617,"user_tz":-120,"elapsed":468,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["And finally we execute it in the workspace"],"metadata":{"id":"NYFD4-kGviQC"}},{"cell_type":"code","source":["# We create a workspace\n","workspace = Workspace()\n","\n","# The temporal agent will be run for 10 steps on this workspace\n","t_agent(workspace, t=0, n_steps=10)\n","\n","# We retrieve the information as they are stored into the workspace\n","obs, action, reward, done = workspace[\"obs\", \"action\", \"reward\", \"done\"]\n","\n","# And we print them\n","print(\"obs:\", obs)\n","print(\"action:\", action)\n","print(\"reward:\", reward)\n","print(\"done:\", done)\n","# You should see that each variable has been recorded for the number of specified \n","# time steps..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uDlS_bMvmmp","executionInfo":{"status":"ok","timestamp":1685512863376,"user_tz":-120,"elapsed":8,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}},"outputId":"c2318569-b545-4c69-cd6c-b0f762c58a14"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["obs: tensor([[0.5653, 0.9000],\n","        [0.1912, 0.8366],\n","        [0.5505, 0.3652],\n","        [0.1115, 0.5979],\n","        [0.7808, 0.6478],\n","        [0.7699, 0.6776],\n","        [0.0700, 0.6624],\n","        [0.0648, 0.5195],\n","        [0.8674, 0.6475],\n","        [0.0628, 0.4697]])\n","action: tensor([[0.2723],\n","        [0.5645],\n","        [0.0146],\n","        [0.3624],\n","        [0.7831],\n","        [0.0625],\n","        [0.9255],\n","        [0.3427],\n","        [0.1136],\n","        [0.0562]])\n","reward: tensor([[0],\n","        [4],\n","        [2],\n","        [4],\n","        [4],\n","        [1],\n","        [2],\n","        [4],\n","        [3],\n","        [1]])\n","done: tensor([[False],\n","        [False],\n","        [False],\n","        [False],\n","        [False],\n","        [False],\n","        [False],\n","        [False],\n","        [False],\n","        [False]])\n"]}]},{"cell_type":"markdown","source":["## What's next?"],"metadata":{"id":"eGzv9OxG-k5Z"}},{"cell_type":"markdown","source":["In [the next notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh?usp=sharing) we will replace these simple random agents with real agents based on neural networks and a real environnement: we will use a neural network ActionAgent and an RL environment from gym to write an elementary RL loop."],"metadata":{"id":"h4BkkTLZ-oJw"}}]}