{"cells":[{"cell_type":"markdown","metadata":{"id":"dYfGJCe52lP4"},"source":["# Outlook"]},{"cell_type":"markdown","metadata":{"id":"aZUSf0n_2otG"},"source":["In [this previous notebook](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh#scrollTo=X3_nZLp8wfjQ), we have seen how to create agents representing gym environments using the NoAutoResetGymAgent class from BBRL. We now explain how to do the same with the AutoResetGymAgent class. "]},{"cell_type":"markdown","source":["This first parts of this notebook are the same as in [the previous one](https://colab.research.google.com/drive/1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh#scrollTo=X3_nZLp8wfjQ), you do not need to read everything again."],"metadata":{"id":"GxHDFq2cx6h3"}},{"cell_type":"markdown","metadata":{"id":"aHO1nIdM21Lq"},"source":["## Installation and imports"]},{"cell_type":"markdown","metadata":{"id":"Ymc-lbXi9vDE"},"source":["The BBRL library is [here](https://github.com/osigaud/bbrl)."]},{"cell_type":"markdown","source":["Note that we install the `my_gym` library to make sure to import the gym version 0.21.0. The interface of the later versions has been modified and maybe incompatible with previous code."],"metadata":{"id":"aOxU5SuSKqFF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0MaggiOl4KU"},"outputs":[],"source":["import functools\n","import time\n","\n","!pip install omegaconf\n","import omegaconf\n","\n","import gym\n","!pip install git+https://github.com/osigaud/my_gym.git\n","\n","try:\n","  import bbrl\n","except ImportError:\n","  from IPython.display import clear_output \n","  !pip install git+https://github.com/osigaud/bbrl.git\n","  clear_output()\n","  import bbrl"]},{"cell_type":"markdown","metadata":{"id":"fE1c7ZLf60X_"},"source":["### BBRL imports"]},{"cell_type":"markdown","source":["As in the previous notebook, we import BBRL agents"],"metadata":{"id":"PrIJUVV32l2f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcuqoAvG3zMZ"},"outputs":[],"source":["from bbrl.workspace import Workspace\n","\n","from bbrl.agents.agent import Agent\n","\n","from bbrl import get_class, get_arguments, instantiate_class\n","\n","# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n","# TemporalAgent(agent) executes an agent (e.g an Agent) over multiple timesteps in the workspace, \n","# or until a given condition is reached\n","from bbrl.agents import Agents, TemporalAgent\n","\n","# GymAgent (resp. AutoResetGymAgent) are agents able to execute a batch of gym environments\n","# without (resp. with) auto-resetting. These agents produce multiple variables in the workspace: \n","# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/done’, ’env/initial_state’, ’env/cumulated_reward’, \n","# ... When called at timestep t=0, then the environments are automatically reset. \n","# At timestep t>0, these agents will read the ’action’ variable in the workspace at time t − 1\n","from bbrl.agents.gyma import AutoResetGymAgent, NoAutoResetGymAgent\n","\n","# Not present in the A2C version...\n","from bbrl.utils.logger import TFLogger"]},{"cell_type":"markdown","metadata":{"id":"m4kV9pWV3wRe"},"source":["### Other Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vktQB-AO5biu"},"outputs":[],"source":["import copy\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import my_gym"]},{"cell_type":"code","source":["from omegaconf import OmegaConf"],"metadata":{"id":"LvWedwMIJWOm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating Neural RL agents"],"metadata":{"id":"AtKPR25b288G"}},{"cell_type":"markdown","source":["We will build two types of agents:\n","- First, a stochastic actor that outputs a stochastic discrete action given a state. This agent will be made of two parts, one for generating probabilities over actions, and the other one for choosing an action according to these probabilities.\n","- Second, a deterministic critic that outputs Q-values for discrete actions given a state. Here we will introduce more general functions to build neural networks from a set of specified layer sizes. "],"metadata":{"id":"63m188vAJ5Bj"}},{"cell_type":"markdown","source":["### A probabilistic actor in two parts"],"metadata":{"id":"29InT1mQK4bl"}},{"cell_type":"markdown","source":["Here we replace the simple ActionAgent of the previous notebook with a combination of two agents: the probabilistic agent, which contains the neural network, and the action agent, which selects an action based on the probabilities resulting from the probabilistic agent."],"metadata":{"id":"PtFXRZRj3dGp"}},{"cell_type":"markdown","source":["#### Probabilistic Agent"],"metadata":{"id":"e3M35WE83ESs"}},{"cell_type":"markdown","source":["A ProbAgent is a one hidden layer neural network which takes an observation as input and whose output is a probability given by a final softmax layer."],"metadata":{"id":"zVhd6r8c4Ihe"}},{"cell_type":"markdown","source":["The first layers are built in the `__init__()` function using the simple `nn.Sequential(...)` model from pytorch. We will do something more sophisticated to deal with an arbitrary number of layers later in another notebook."],"metadata":{"id":"dDhvWRrQ6iHV"}},{"cell_type":"markdown","source":["Now, let us have a look at the `forward()` function, which is called each time the agent performs a step in the environment."],"metadata":{"id":"-URAC2Cz6-58"}},{"cell_type":"markdown","source":["To get the input observation from the environment we call\n","`observation = self.get((\"env/env_obs\", t))`\n","and that to perform an action in the environment we call\n","`self.set((\"action_probs\", t), probs)`. In between, we call `torch.softmax()` to get probabilities from the output layer of the network."],"metadata":{"id":"GTFr51h44Jod"}},{"cell_type":"code","source":["class ProbAgent(Agent):\n","    def __init__(self, observation_size, hidden_size, n_actions):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(observation_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, n_actions),\n","        )\n","\n","    def forward(self, t, **kwargs):\n","        observation = self.get((\"env/env_obs\", t))\n","        scores = self.model(observation)\n","        probs = torch.softmax(scores, dim=-1)\n","        self.set((\"action_probs\", t), probs)"],"metadata":{"id":"3aFquLLO3Zeo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Actor Agent"],"metadata":{"id":"OlOJSlR83KCE"}},{"cell_type":"markdown","source":["The ActorAgent takes action probabilities as input (coming from the ProbAgent) and outputs an action. In the deterministic case it takes the argmax, in the stochastic case it samples from the Categorical distribution. This agent does not have a neural network, it just takes a decision from the output of the ProbAgent."],"metadata":{"id":"gRq0XvYA4-SA"}},{"cell_type":"code","source":["class ActorAgent(Agent):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, t, stochastic, **kwargs):\n","        probs = self.get((\"action_probs\", t))\n","        if stochastic:\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = probs.argmax(1)\n","\n","        self.set((\"action\", t), action)"],"metadata":{"id":"QgpXfN1e4_gs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that instead of having a ProbaAgent then an ActorAgent, we could have built a single agent containing both. We are doing this mainly to illustrate the capabilities of BBRL to combine agents."],"metadata":{"id":"OW0hRb605iH-"}},{"cell_type":"markdown","source":["Note also that this pair of agents is adequate for environment with discrete actions, but we need something different if the environment takes continuous actions. In that case, instead of a categorical distribution, we would rather use a Gaussian distribution. We will build other versions of these agents later in another notebook."],"metadata":{"id":"4lTRFolx5s9z"}},{"cell_type":"markdown","source":["### A deterministic critic agent"],"metadata":{"id":"84aMsnrELCoE"}},{"cell_type":"markdown","source":["The function below builds a multi-layer perceptron where the size of each layer is given in the `size` list. We also specify the activation function of neurons at each layer and optionally a different activation function for the final layer."],"metadata":{"id":"l_LkwKoEMPsX"}},{"cell_type":"code","source":["def build_mlp(sizes, activation, output_activation=nn.Identity()):\n","    layers = []\n","    for j in range(len(sizes) - 1):\n","        act = activation if j < len(sizes) - 2 else output_activation\n","        layers += [nn.Linear(sizes[j], sizes[j + 1]), act]\n","    return nn.Sequential(*layers)"],"metadata":{"id":"kOmwets4LZub"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `DiscreteQAgent` class implements a critic such the one used in DQN. It has one output neuron per action and its output is the Q-value of these actions given the state. "],"metadata":{"id":"6u8r5T5sMppN"}},{"cell_type":"markdown","source":["Note that as any BBRL agent, it has a forward function that takes a time state as input. This forward function outputs the Q-values at the corresponding time step. Additionally, if the critic is used to choose an action, it also outputs the chosen action at the same time step."],"metadata":{"id":"T_Rj1SZrNB6E"}},{"cell_type":"markdown","source":["Besides, it is also useful to get the network output (as a Q-value or as an action) given a state rather than a time step. This is what the `predict_action` and `predict_value` functions are used for."],"metadata":{"id":"BdNNM9B7Naz4"}},{"cell_type":"code","source":["class DiscreteQAgent(Agent):\n","    def __init__(self, state_dim, hidden_layers, action_dim):\n","        super().__init__()\n","        self.is_q_function = True\n","        self.model = build_mlp(\n","            [state_dim] + list(hidden_layers) + [action_dim], activation=nn.ReLU()\n","        )\n","\n","    def forward(self, t, choose_action=True, **kwargs):\n","        obs = self.get((\"env/env_obs\", t))\n","        q_values = self.model(obs).squeeze(-1)\n","        self.set((\"q_values\", t), q_values)\n","        if choose_action:\n","            action = q_values.argmax(1)\n","            self.set((\"action\", t), action)\n","\n","    def predict_action(self, obs, stochastic):\n","        q_values = self.model(obs).squeeze(-1)\n","        if stochastic:\n","            probs = torch.softmax(q_values, dim=-1)\n","            action = torch.distributions.Categorical(probs).sample()\n","        else:\n","            action = q_values.argmax(0)\n","        return action\n","\n","    def predict_value(self, obs, action):\n","        q_values = self.model(obs).squeeze(-1)\n","        return q_values[0][action]"],"metadata":{"id":"O4u_vhwjLIfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01yjyr34OU4-"},"source":["## Creating the environment agent"]},{"cell_type":"markdown","source":["Now that we have an RL agent, let us create an environment. In the previous notebook, we were doing something simple building on the Agent class. Now we do something more sophisticated, building on the `GymAgent` class and encapsulating an OpenAI gym environment."],"metadata":{"id":"mk0wEvfC-97e"}},{"cell_type":"markdown","metadata":{"id":"BHaRqpSAGYDL"},"source":["### Using a gym environment"]},{"cell_type":"markdown","metadata":{"id":"O-7OSw9BGb7t"},"source":["The function below creates the environment. In OpenAI gym, an environment can be known by its name, here the string `env_name`. The environment is generally given a maximum number of steps, which is enforced by the `TimeLimit` wrapper. Therefore, we we do not need to add our own TimeLimit: this may break the episode termination behavior."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fsb5QRzw7V0o"},"outputs":[],"source":["def make_env(env_name):\n","    return gym.make(env_name)"]},{"cell_type":"markdown","source":["To call the above function, we will use a reflexive instantiation mechanism and get the parameters of the function from the `params` dictionary, in the `\"env\":{\n","      \"classname\": \"__main__.make_env\",\n","      \"env_name\": \"CartPole-v1\",\n","    }` part."],"metadata":{"id":"QA7b5PigC7JQ"}},{"cell_type":"markdown","source":["Using this instantiation approach from a function is useful if you define a new env for instance i.e you just change the 'classname' and put the arguments of the constructor directly and everything will work fine. This may be not natural a first sight, but if you start to use it, you will never go back again :) "],"metadata":{"id":"E448hsHtDcX6"}},{"cell_type":"markdown","source":["The `instantiate_class`, `get_class` and `get_arguments` functions are available in the [`main/bbrl/__init__.py`](https://github.com/osigaud/bbrl/blob/master/bbrl/__init__.py) file. The `get_class` function reads the `classname` in the parameters to create the appropriate type of object, and the `get_arguments` function reads the local paremeters and their values to set them into the corresponding object. "],"metadata":{"id":"VFmPRnp4DlUH"}},{"cell_type":"markdown","source":["## Running several episodes split into epochs with an AutoReset environment"],"metadata":{"id":"AplNw1tU4U4Z"}},{"cell_type":"markdown","source":["The `NoAutoResetGymAgent` is the easiest environment to use. Let us now consider the `AutoResetGymAgent`, that we will use to run several episodes split into epochs. This more complicated type of environment is explained in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5?usp=sharing)."],"metadata":{"id":"4d1m736_4kC5"}},{"cell_type":"markdown","source":["### Creating the environment agent"],"metadata":{"id":"qFvULWPx4-DM"}},{"cell_type":"markdown","source":["This is as before, but we just use an `AutoResetGymAgent` instead of a `NoAutoResetGymAgent` one."],"metadata":{"id":"9OwhF7q95DC_"}},{"cell_type":"code","source":["def get_env(cfg):\n","    env_agent = AutoResetGymAgent(\n","        get_class(cfg.gym_env),\n","        get_arguments(cfg.gym_env),\n","        cfg.algorithm.n_envs,\n","        cfg.algorithm.seed,\n","    )\n","    return env_agent"],"metadata":{"id":"VCTg7vww5O7u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building the temporal agent"],"metadata":{"id":"xaX9cSjE6DTS"}},{"cell_type":"markdown","source":["Creating the temporal agent is exactly as before."],"metadata":{"id":"yjyejHeb6GfE"}},{"cell_type":"markdown","source":["In the parameters, we add a number of epochs (here 5), a number of steps per epoch (here 10) and this time we use several environments in parallel (here 3)."],"metadata":{"id":"WgVO-czC7cga"}},{"cell_type":"code","source":["params2={\n","  \"algorithm\":{\n","    \"seed\": 432,\n","    \"nb_epochs\": 5,\n","    \"n_steps\": 10,\n","    \"n_envs\": 3,\n","    \"architecture\":{\"hidden_size\": 32},\n","  },\n","  \"gym_env\":{\n","    \"classname\": \"__main__.make_env\",\n","    \"env_name\": \"CartPole-v1\",\n","  },\n","}"],"metadata":{"id":"cEt_6qJR7Hch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = OmegaConf.create(params2)\n","\n","env_agent = get_env(config)\n","observation_size, n_actions = env_agent.get_obs_and_actions_sizes()\n","prob_agent = ProbAgent(observation_size, config.algorithm.architecture.hidden_size, n_actions)\n","action_agent = ActorAgent()\n","composed_agent = Agents(env_agent, prob_agent, action_agent)\n","  \n","# Get a temporal agent that can be executed in a workspace\n","t_agent = TemporalAgent(composed_agent)"],"metadata":{"id":"TO3F2eYY6R_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Running the main loop"],"metadata":{"id":"a3t9cyHg5qq1"}},{"cell_type":"markdown","source":["We now write the main loop to run a number of epochs specified in the params. There are three parts inside the loop over epochs.\n","\n","- In the first part, the temporal agent is run in the workspace for each epoch. Note that the first epoch is different, as in the next epochs we need to copy the last step of the previous epoch to avoid missing a transition. This is explained in detail in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\n","\n","- The second part consists in collecting the interaction data from the workspace. Note that to properly filter out the transitions from an episode to the next, we have to rearrange the data using the `get_transitions()` function. As a result, the data structure we get back from the workspace are a little more complicated. Again, this is explained in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5).\n","\n","- The third part simply consists in counting the steps. One must not forget to multiply by the number of environment. One cannot simply use cfg.algorithm.n_step` as transition from an episode to the next are filtered out, as explained in [this notebook](https://colab.research.google.com/drive/1W9Y-3fa6LsPeR6cBC1vgwBjKfgMwZvP5)."],"metadata":{"id":"9VAjFOeT52mU"}},{"cell_type":"code","source":["workspace = Workspace()\n","\n","nb_steps = 0\n","for epoch in range(config.algorithm.nb_epochs):\n","        if epoch == 0:\n","            t_agent(workspace, t=0, n_steps=config.algorithm.n_steps, stochastic=True)\n","        else:\n","            workspace.zero_grad()\n","            workspace.copy_n_last_steps(1)\n","            t_agent(workspace, t=1, n_steps=config.algorithm.n_steps - 1, stochastic=True)\n","\n","\n","        transition_workspace = workspace.get_transitions()\n","\n","        # We retrieve the information as they are stored into the workspace\n","        obs, done, truncated, reward, action = transition_workspace[\n","            \"env/env_obs\", \"env/done\", \"env/truncated\", \"env/reward\", \"action\"\n","        ]\n","\n","        nb_steps += len(action[0]) * cfg.algorithm.n_envs\n","        # And we print them\n","        print(\"obs:\", obs)\n","        print(\"action:\", action)\n","        print(\"reward:\", reward)\n","        print(\"done:\", done)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wdNeC6g6Y-t","executionInfo":{"status":"ok","timestamp":1654704508042,"user_tz":-120,"elapsed":307,"user":{"displayName":"Olivier Sigaud","userId":"07951496868143636810"}},"outputId":"8fd9184d-5a22-42be-9614-a9efc0f3f760"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["obs: tensor([[[-3.0960e-02,  4.1176e-02,  1.7323e-02, -4.8666e-02],\n","         [-4.1527e-02, -3.5080e-02,  1.5423e-02, -3.1612e-02],\n","         [-4.6641e-02,  1.5900e-02,  4.9895e-02,  5.6147e-03],\n","         [-3.0137e-02, -1.5419e-01,  1.6350e-02,  2.4943e-01],\n","         [-4.2229e-02,  1.5982e-01,  1.4791e-02, -3.1939e-01],\n","         [-4.6323e-02, -1.7990e-01,  5.0007e-02,  3.1361e-01],\n","         [-3.3221e-02,  4.0695e-02,  2.1339e-02, -3.8049e-02],\n","         [-3.9032e-02,  3.5473e-01,  8.4034e-03, -6.0737e-01],\n","         [-4.9921e-02, -3.7570e-01,  5.6280e-02,  6.2164e-01],\n","         [-3.2407e-02,  2.3550e-01,  2.0578e-02, -3.2392e-01],\n","         [-3.1938e-02,  1.5949e-01, -3.7440e-03, -3.1205e-01],\n","         [-5.7435e-02, -5.7156e-01,  6.8713e-02,  9.3150e-01],\n","         [-2.7697e-02,  4.3033e-01,  1.4099e-02, -6.1005e-01],\n","         [-2.8748e-02,  3.5466e-01, -9.9851e-03, -6.0591e-01],\n","         [-6.8866e-02, -7.6754e-01,  8.7343e-02,  1.2450e+00],\n","         [-1.9090e-02,  6.2525e-01,  1.8981e-03, -8.9826e-01],\n","         [-2.1655e-02,  1.5968e-01, -2.2103e-02, -3.1639e-01],\n","         [-8.4217e-02, -9.6366e-01,  1.1224e-01,  1.5637e+00],\n","         [-6.5850e-03,  8.2035e-01, -1.6067e-02, -1.1903e+00],\n","         [-1.8461e-02, -3.5119e-02, -2.8431e-02, -3.0762e-02],\n","         [-1.0349e-01, -7.7005e-01,  1.4352e-01,  1.3080e+00],\n","         [ 9.8219e-03,  1.0157e+00, -3.9874e-02, -1.4880e+00],\n","         [-1.9164e-02, -2.2982e-01, -2.9046e-02,  2.5282e-01],\n","         [-1.1889e-01, -5.7701e-01,  1.6968e-01,  1.0635e+00],\n","         [ 3.0135e-02,  1.2113e+00, -6.9634e-02, -1.7929e+00],\n","         [-2.3760e-02, -4.2452e-01, -2.3990e-02,  5.3620e-01],\n","         [-1.3043e-01, -7.7392e-01,  1.9095e-01,  1.4042e+00]],\n","\n","        [[-3.0137e-02, -1.5419e-01,  1.6350e-02,  2.4943e-01],\n","         [-4.2229e-02,  1.5982e-01,  1.4791e-02, -3.1939e-01],\n","         [-4.6323e-02, -1.7990e-01,  5.0007e-02,  3.1361e-01],\n","         [-3.3221e-02,  4.0695e-02,  2.1339e-02, -3.8049e-02],\n","         [-3.9032e-02,  3.5473e-01,  8.4034e-03, -6.0737e-01],\n","         [-4.9921e-02, -3.7570e-01,  5.6280e-02,  6.2164e-01],\n","         [-3.2407e-02,  2.3550e-01,  2.0578e-02, -3.2392e-01],\n","         [-3.1938e-02,  1.5949e-01, -3.7440e-03, -3.1205e-01],\n","         [-5.7435e-02, -5.7156e-01,  6.8713e-02,  9.3150e-01],\n","         [-2.7697e-02,  4.3033e-01,  1.4099e-02, -6.1005e-01],\n","         [-2.8748e-02,  3.5466e-01, -9.9851e-03, -6.0591e-01],\n","         [-6.8866e-02, -7.6754e-01,  8.7343e-02,  1.2450e+00],\n","         [-1.9090e-02,  6.2525e-01,  1.8981e-03, -8.9826e-01],\n","         [-2.1655e-02,  1.5968e-01, -2.2103e-02, -3.1639e-01],\n","         [-8.4217e-02, -9.6366e-01,  1.1224e-01,  1.5637e+00],\n","         [-6.5850e-03,  8.2035e-01, -1.6067e-02, -1.1903e+00],\n","         [-1.8461e-02, -3.5119e-02, -2.8431e-02, -3.0762e-02],\n","         [-1.0349e-01, -7.7005e-01,  1.4352e-01,  1.3080e+00],\n","         [ 9.8219e-03,  1.0157e+00, -3.9874e-02, -1.4880e+00],\n","         [-1.9164e-02, -2.2982e-01, -2.9046e-02,  2.5282e-01],\n","         [-1.1889e-01, -5.7701e-01,  1.6968e-01,  1.0635e+00],\n","         [ 3.0135e-02,  1.2113e+00, -6.9634e-02, -1.7929e+00],\n","         [-2.3760e-02, -4.2452e-01, -2.3990e-02,  5.3620e-01],\n","         [-1.3043e-01, -7.7392e-01,  1.9095e-01,  1.4042e+00],\n","         [ 5.4360e-02,  1.4071e+00, -1.0549e-01, -2.1064e+00],\n","         [-3.2251e-02, -2.2907e-01, -1.3266e-02,  2.3605e-01],\n","         [-1.4591e-01, -9.7083e-01,  2.1903e-01,  1.7500e+00]]])\n","action: tensor([[0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n","         1, 1, 0],\n","        [1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n","         1, 1, 0]])\n","reward: tensor([[0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n","done: tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False],\n","        [False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False,  True]])\n","obs: tensor([[[ 5.4360e-02,  1.4071e+00, -1.0549e-01, -2.1064e+00],\n","         [-3.2251e-02, -2.2907e-01, -1.3266e-02,  2.3605e-01],\n","         [ 8.2502e-02,  1.6031e+00, -1.4762e-01, -2.4297e+00],\n","         [-3.6832e-02, -3.3757e-02, -8.5451e-03, -6.0784e-02],\n","         [ 3.2527e-02,  3.5941e-02,  4.3087e-02,  2.3508e-02],\n","         [ 1.1456e-01,  1.4095e+00, -1.9621e-01, -2.1857e+00],\n","         [-3.7507e-02,  1.6149e-01, -9.7608e-03, -3.5615e-01],\n","         [ 3.3246e-02, -1.5977e-01,  4.3558e-02,  3.2947e-01],\n","         [-3.4277e-02, -3.3496e-02, -1.6884e-02, -6.6562e-02],\n","         [ 3.0051e-02, -3.5549e-01,  5.0147e-02,  6.3556e-01],\n","         [ 4.1164e-02, -3.0842e-02,  4.7674e-02,  8.4361e-03],\n","         [-3.4947e-02, -2.2837e-01, -1.8215e-02,  2.2075e-01],\n","         [ 2.2941e-02, -5.5127e-01,  6.2858e-02,  9.4361e-01],\n","         [ 4.0547e-02, -2.2661e-01,  4.7843e-02,  3.1577e-01],\n","         [-3.9515e-02, -3.2994e-02, -1.3800e-02, -7.7626e-02],\n","         [ 1.1916e-02, -3.5705e-01,  8.1730e-02,  6.7132e-01],\n","         [ 3.6015e-02, -4.2238e-01,  5.4158e-02,  6.2315e-01],\n","         [-4.0174e-02,  1.6232e-01, -1.5353e-02, -3.7463e-01],\n","         [ 4.7748e-03, -1.6315e-01,  9.5157e-02,  4.0545e-01],\n","         [ 2.7567e-02, -6.1822e-01,  6.6621e-02,  9.3239e-01],\n","         [-3.6928e-02,  3.5766e-01, -2.2845e-02, -6.7211e-01],\n","         [ 1.5117e-03,  3.0501e-02,  1.0327e-01,  1.4422e-01],\n","         [ 1.5203e-02, -8.1417e-01,  8.5269e-02,  1.2452e+00],\n","         [-2.9775e-02,  5.5309e-01, -3.6288e-02, -9.7190e-01],\n","         [ 2.1217e-03,  2.2400e-01,  1.0615e-01, -1.1419e-01]],\n","\n","        [[ 8.2502e-02,  1.6031e+00, -1.4762e-01, -2.4297e+00],\n","         [-3.6832e-02, -3.3757e-02, -8.5451e-03, -6.0784e-02],\n","         [ 1.1456e-01,  1.4095e+00, -1.9621e-01, -2.1857e+00],\n","         [-3.7507e-02,  1.6149e-01, -9.7608e-03, -3.5615e-01],\n","         [ 3.3246e-02, -1.5977e-01,  4.3558e-02,  3.2947e-01],\n","         [ 1.4275e-01,  1.6059e+00, -2.3993e-01, -2.5320e+00],\n","         [-3.4277e-02, -3.3496e-02, -1.6884e-02, -6.6562e-02],\n","         [ 3.0051e-02, -3.5549e-01,  5.0147e-02,  6.3556e-01],\n","         [-3.4947e-02, -2.2837e-01, -1.8215e-02,  2.2075e-01],\n","         [ 2.2941e-02, -5.5127e-01,  6.2858e-02,  9.4361e-01],\n","         [ 4.0547e-02, -2.2661e-01,  4.7843e-02,  3.1577e-01],\n","         [-3.9515e-02, -3.2994e-02, -1.3800e-02, -7.7626e-02],\n","         [ 1.1916e-02, -3.5705e-01,  8.1730e-02,  6.7132e-01],\n","         [ 3.6015e-02, -4.2238e-01,  5.4158e-02,  6.2315e-01],\n","         [-4.0174e-02,  1.6232e-01, -1.5353e-02, -3.7463e-01],\n","         [ 4.7748e-03, -1.6315e-01,  9.5157e-02,  4.0545e-01],\n","         [ 2.7567e-02, -6.1822e-01,  6.6621e-02,  9.3239e-01],\n","         [-3.6928e-02,  3.5766e-01, -2.2845e-02, -6.7211e-01],\n","         [ 1.5117e-03,  3.0501e-02,  1.0327e-01,  1.4422e-01],\n","         [ 1.5203e-02, -8.1417e-01,  8.5269e-02,  1.2452e+00],\n","         [-2.9775e-02,  5.5309e-01, -3.6288e-02, -9.7190e-01],\n","         [ 2.1217e-03,  2.2400e-01,  1.0615e-01, -1.1419e-01],\n","         [-1.0805e-03, -1.0103e+00,  1.1017e-01,  1.5634e+00],\n","         [-1.8713e-02,  7.4868e-01, -5.5726e-02, -1.2758e+00],\n","         [ 6.6018e-03,  2.7534e-02,  1.0387e-01,  2.1001e-01]]])\n","action: tensor([[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n","         0],\n","        [0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n","         0]])\n","reward: tensor([[1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1.]])\n","done: tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False],\n","        [False, False, False, False, False,  True, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False]])\n","obs: tensor([[[-1.0805e-03, -1.0103e+00,  1.1017e-01,  1.5634e+00],\n","         [-1.8713e-02,  7.4868e-01, -5.5726e-02, -1.2758e+00],\n","         [ 6.6018e-03,  2.7534e-02,  1.0387e-01,  2.1001e-01],\n","         [-2.1286e-02, -1.2065e+00,  1.4144e-01,  1.8883e+00],\n","         [-3.7394e-03,  5.5431e-01, -8.1241e-02, -1.0010e+00],\n","         [ 7.1525e-03, -1.6891e-01,  1.0807e-01,  5.3357e-01],\n","         [-4.5417e-02, -1.4029e+00,  1.7921e-01,  2.2213e+00],\n","         [ 7.3469e-03,  7.5042e-01, -1.0126e-01, -1.3181e+00],\n","         [ 3.7743e-03, -3.6537e-01,  1.1874e-01,  8.5825e-01],\n","         [ 2.2355e-02,  5.5671e-01, -1.2762e-01, -1.0587e+00],\n","         [-3.5331e-03, -1.7205e-01,  1.3590e-01,  6.0514e-01],\n","         [-2.9414e-02, -2.1532e-02,  4.1918e-02,  5.6715e-03],\n","         [ 3.3490e-02,  7.5327e-01, -1.4880e-01, -1.3886e+00],\n","         [-6.9741e-03,  2.0937e-02,  1.4801e-01,  3.5816e-01],\n","         [-2.9845e-02,  1.7296e-01,  4.2031e-02, -2.7350e-01],\n","         [ 4.8555e-02,  9.4990e-01, -1.7657e-01, -1.7239e+00],\n","         [-6.5554e-03, -1.7595e-01,  1.5517e-01,  6.9361e-01],\n","         [-2.6385e-02, -2.2732e-02,  3.6562e-02,  3.2141e-02],\n","         [-1.0074e-02,  1.6723e-02,  1.6904e-01,  4.5352e-01],\n","         [-2.6840e-02, -2.1836e-01,  3.7204e-02,  3.3613e-01],\n","         [-6.2938e-03, -1.3472e-02,  3.3727e-03, -1.5797e-02],\n","         [-9.7398e-03, -1.8034e-01,  1.7811e-01,  7.9435e-01],\n","         [-3.1207e-02, -2.3785e-02,  4.3927e-02,  5.5409e-02],\n","         [-6.5632e-03,  1.8160e-01,  3.0567e-03, -3.0741e-01],\n","         [-1.3347e-02,  1.1953e-02,  1.9400e-01,  5.6257e-01]],\n","\n","        [[-2.1286e-02, -1.2065e+00,  1.4144e-01,  1.8883e+00],\n","         [-3.7394e-03,  5.5431e-01, -8.1241e-02, -1.0010e+00],\n","         [ 7.1525e-03, -1.6891e-01,  1.0807e-01,  5.3357e-01],\n","         [-4.5417e-02, -1.4029e+00,  1.7921e-01,  2.2213e+00],\n","         [ 7.3469e-03,  7.5042e-01, -1.0126e-01, -1.3181e+00],\n","         [ 3.7743e-03, -3.6537e-01,  1.1874e-01,  8.5825e-01],\n","         [-7.3474e-02, -1.5992e+00,  2.2363e-01,  2.5635e+00],\n","         [ 2.2355e-02,  5.5671e-01, -1.2762e-01, -1.0587e+00],\n","         [-3.5331e-03, -1.7205e-01,  1.3590e-01,  6.0514e-01],\n","         [ 3.3490e-02,  7.5327e-01, -1.4880e-01, -1.3886e+00],\n","         [-6.9741e-03,  2.0937e-02,  1.4801e-01,  3.5816e-01],\n","         [-2.9845e-02,  1.7296e-01,  4.2031e-02, -2.7350e-01],\n","         [ 4.8555e-02,  9.4990e-01, -1.7657e-01, -1.7239e+00],\n","         [-6.5554e-03, -1.7595e-01,  1.5517e-01,  6.9361e-01],\n","         [-2.6385e-02, -2.2732e-02,  3.6562e-02,  3.2141e-02],\n","         [ 6.7553e-02,  7.5719e-01, -2.1105e-01, -1.4909e+00],\n","         [-1.0074e-02,  1.6723e-02,  1.6904e-01,  4.5352e-01],\n","         [-2.6840e-02, -2.1836e-01,  3.7204e-02,  3.3613e-01],\n","         [-9.7398e-03, -1.8034e-01,  1.7811e-01,  7.9435e-01],\n","         [-3.1207e-02, -2.3785e-02,  4.3927e-02,  5.5409e-02],\n","         [-6.5632e-03,  1.8160e-01,  3.0567e-03, -3.0741e-01],\n","         [-1.3347e-02,  1.1953e-02,  1.9400e-01,  5.6257e-01],\n","         [-3.1683e-02,  1.7068e-01,  4.5035e-02, -2.2310e-01],\n","         [-2.9312e-03,  3.7668e-01, -3.0915e-03, -5.9913e-01],\n","         [-1.3107e-02,  2.0390e-01,  2.0525e-01,  3.3673e-01]]])\n","action: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n","         1],\n","        [0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n","         1]])\n","reward: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 0., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1.]])\n","done: tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False],\n","        [False, False, False, False, False, False,  True, False, False, False,\n","         False, False, False, False, False,  True, False, False, False, False,\n","         False, False, False, False, False]])\n","obs: tensor([[[-3.1683e-02,  1.7068e-01,  4.5035e-02, -2.2310e-01],\n","         [-2.9312e-03,  3.7668e-01, -3.0915e-03, -5.9913e-01],\n","         [-1.3107e-02,  2.0390e-01,  2.0525e-01,  3.3673e-01],\n","         [-2.8269e-02, -2.5055e-02,  4.0573e-02,  8.3445e-02],\n","         [ 4.6024e-03,  5.7185e-01, -1.5074e-02, -8.9279e-01],\n","         [-2.8770e-02,  1.6946e-01,  4.2242e-02, -1.9617e-01],\n","         [ 1.6039e-02,  7.6717e-01, -3.2930e-02, -1.1902e+00],\n","         [ 3.6241e-02,  1.2496e-02,  9.2289e-03, -4.1084e-02],\n","         [-2.5381e-02, -2.6238e-02,  3.8319e-02,  1.0954e-01],\n","         [ 3.1383e-02,  5.7249e-01, -5.6733e-02, -9.0799e-01],\n","         [ 3.6491e-02,  2.0748e-01,  8.4072e-03, -3.3084e-01],\n","         [-2.5906e-02,  1.6831e-01,  4.0510e-02, -1.7081e-01],\n","         [ 4.2832e-02,  7.6833e-01, -7.4893e-02, -1.2179e+00],\n","         [ 4.0641e-02,  1.2244e-02,  1.7904e-03, -3.5518e-02],\n","         [-2.2540e-02,  3.6283e-01,  3.7093e-02, -4.5045e-01],\n","         [ 5.8199e-02,  5.7425e-01, -9.9252e-02, -9.4964e-01],\n","         [ 4.0886e-02, -1.8290e-01,  1.0800e-03,  2.5773e-01],\n","         [-1.5283e-02,  5.5741e-01,  2.8084e-02, -7.3121e-01],\n","         [ 6.9684e-02,  7.7056e-01, -1.1824e-01, -1.2718e+00],\n","         [ 3.7228e-02, -3.7804e-01,  6.2346e-03,  5.5075e-01],\n","         [-4.1346e-03,  7.5214e-01,  1.3460e-02, -1.0149e+00],\n","         [ 8.5095e-02,  5.7713e-01, -1.4368e-01, -1.0183e+00],\n","         [ 2.9667e-02, -1.8301e-01,  1.7250e-02,  2.6004e-01],\n","         [ 1.0908e-02,  5.5684e-01, -6.8384e-03, -7.1805e-01],\n","         [ 9.6638e-02,  3.8418e-01, -1.6405e-01, -7.7401e-01],\n","         [ 2.6007e-02,  1.1864e-02,  2.2450e-02, -2.7153e-02]],\n","\n","        [[-2.8269e-02, -2.5055e-02,  4.0573e-02,  8.3445e-02],\n","         [ 4.6024e-03,  5.7185e-01, -1.5074e-02, -8.9279e-01],\n","         [-9.0295e-03,  3.9560e-01,  2.1198e-01,  1.1513e-01],\n","         [-2.8770e-02,  1.6946e-01,  4.2242e-02, -1.9617e-01],\n","         [ 1.6039e-02,  7.6717e-01, -3.2930e-02, -1.1902e+00],\n","         [-2.5381e-02, -2.6238e-02,  3.8319e-02,  1.0954e-01],\n","         [ 3.1383e-02,  5.7249e-01, -5.6733e-02, -9.0799e-01],\n","         [ 3.6491e-02,  2.0748e-01,  8.4072e-03, -3.3084e-01],\n","         [-2.5906e-02,  1.6831e-01,  4.0510e-02, -1.7081e-01],\n","         [ 4.2832e-02,  7.6833e-01, -7.4893e-02, -1.2179e+00],\n","         [ 4.0641e-02,  1.2244e-02,  1.7904e-03, -3.5518e-02],\n","         [-2.2540e-02,  3.6283e-01,  3.7093e-02, -4.5045e-01],\n","         [ 5.8199e-02,  5.7425e-01, -9.9252e-02, -9.4964e-01],\n","         [ 4.0886e-02, -1.8290e-01,  1.0800e-03,  2.5773e-01],\n","         [-1.5283e-02,  5.5741e-01,  2.8084e-02, -7.3121e-01],\n","         [ 6.9684e-02,  7.7056e-01, -1.1824e-01, -1.2718e+00],\n","         [ 3.7228e-02, -3.7804e-01,  6.2346e-03,  5.5075e-01],\n","         [-4.1346e-03,  7.5214e-01,  1.3460e-02, -1.0149e+00],\n","         [ 8.5095e-02,  5.7713e-01, -1.4368e-01, -1.0183e+00],\n","         [ 2.9667e-02, -1.8301e-01,  1.7250e-02,  2.6004e-01],\n","         [ 1.0908e-02,  5.5684e-01, -6.8384e-03, -7.1805e-01],\n","         [ 9.6638e-02,  3.8418e-01, -1.6405e-01, -7.7401e-01],\n","         [ 2.6007e-02,  1.1864e-02,  2.2450e-02, -2.7153e-02],\n","         [ 2.2045e-02,  3.6181e-01, -2.1199e-02, -4.2752e-01],\n","         [ 1.0432e-01,  5.8113e-01, -1.7953e-01, -1.1135e+00],\n","         [ 2.6244e-02, -1.8357e-01,  2.1907e-02,  2.7253e-01]]])\n","action: tensor([[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n","         1, 0],\n","        [1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n","         0, 0]])\n","reward: tensor([[1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1.]])\n","done: tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False],\n","        [False, False,  True, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False]])\n","obs: tensor([[[ 0.0220,  0.3618, -0.0212, -0.4275],\n","         [ 0.1043,  0.5811, -0.1795, -1.1135],\n","         [ 0.0262, -0.1836,  0.0219,  0.2725],\n","         [ 0.0293,  0.5572, -0.0297, -0.7268],\n","         [ 0.1159,  0.3888, -0.2018, -0.8821],\n","         [ 0.0226, -0.3790,  0.0274,  0.5720],\n","         [ 0.0404,  0.3625, -0.0443, -0.4436],\n","         [ 0.0150, -0.1843,  0.0388,  0.2881],\n","         [ 0.0477,  0.5582, -0.0532, -0.7499],\n","         [-0.0170,  0.0240,  0.0342, -0.0250],\n","         [ 0.0113,  0.0103,  0.0446,  0.0079],\n","         [ 0.0588,  0.3639, -0.0682, -0.4745],\n","         [-0.0165, -0.1716,  0.0337,  0.2783],\n","         [ 0.0115,  0.2047,  0.0447, -0.2704],\n","         [ 0.0661,  0.1698, -0.0776, -0.2040],\n","         [-0.0199, -0.3672,  0.0393,  0.5814],\n","         [ 0.0156,  0.3992,  0.0393, -0.5486],\n","         [ 0.0695,  0.3659, -0.0817, -0.5201],\n","         [-0.0273, -0.1726,  0.0509,  0.3014],\n","         [ 0.0236,  0.5937,  0.0283, -0.8287],\n","         [ 0.0768,  0.5621, -0.0921, -0.8374],\n","         [-0.0307, -0.3684,  0.0569,  0.6097],\n","         [ 0.0355,  0.7885,  0.0118, -1.1123],\n","         [ 0.0881,  0.7584, -0.1089, -1.1576],\n","         [-0.0381, -0.5643,  0.0691,  0.9197],\n","         [ 0.0512,  0.5932, -0.0105, -0.8160]],\n","\n","        [[ 0.0293,  0.5572, -0.0297, -0.7268],\n","         [ 0.1159,  0.3888, -0.2018, -0.8821],\n","         [ 0.0226, -0.3790,  0.0274,  0.5720],\n","         [ 0.0404,  0.3625, -0.0443, -0.4436],\n","         [ 0.1237,  0.5860, -0.2194, -1.2308],\n","         [ 0.0150, -0.1843,  0.0388,  0.2881],\n","         [ 0.0477,  0.5582, -0.0532, -0.7499],\n","         [ 0.0113,  0.0103,  0.0446,  0.0079],\n","         [ 0.0588,  0.3639, -0.0682, -0.4745],\n","         [-0.0165, -0.1716,  0.0337,  0.2783],\n","         [ 0.0115,  0.2047,  0.0447, -0.2704],\n","         [ 0.0661,  0.1698, -0.0776, -0.2040],\n","         [-0.0199, -0.3672,  0.0393,  0.5814],\n","         [ 0.0156,  0.3992,  0.0393, -0.5486],\n","         [ 0.0695,  0.3659, -0.0817, -0.5201],\n","         [-0.0273, -0.1726,  0.0509,  0.3014],\n","         [ 0.0236,  0.5937,  0.0283, -0.8287],\n","         [ 0.0768,  0.5621, -0.0921, -0.8374],\n","         [-0.0307, -0.3684,  0.0569,  0.6097],\n","         [ 0.0355,  0.7885,  0.0118, -1.1123],\n","         [ 0.0881,  0.7584, -0.1089, -1.1576],\n","         [-0.0381, -0.5643,  0.0691,  0.9197],\n","         [ 0.0512,  0.5932, -0.0105, -0.8160],\n","         [ 0.1032,  0.9547, -0.1320, -1.4823],\n","         [-0.0494, -0.7603,  0.0875,  1.2333],\n","         [ 0.0631,  0.3982, -0.0268, -0.5266]]])\n","action: tensor([[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n","         0, 0],\n","        [0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n","         0, 1]])\n","reward: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","         1., 1., 1., 1., 1., 1., 1., 1.]])\n","done: tensor([[False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False],\n","        [False, False, False, False,  True, False, False, False, False, False,\n","         False, False, False, False, False, False, False, False, False, False,\n","         False, False, False, False, False, False]])\n"]}]},{"cell_type":"markdown","source":["### Understanding the stored data"],"metadata":{"id":"YLfPdyTRDqWk"}},{"cell_type":"markdown","source":["Exercises: \n","- how do we get the reward of the current time step?\n","- how do we get the observation of the next time step?"],"metadata":{"id":"7B-0eEQHD0Eq"}},{"cell_type":"markdown","source":["## What's next?"],"metadata":{"id":"paHdoNlz9Lpg"}},{"cell_type":"markdown","source":["We are now ready to write a version of the DQN algorithm with the AutoResetGymAgent. We do so in [this notebook](https://colab.research.google.com/drive/1H9_gkenmb_APnbygme1oEdhqMLSDc_bM?usp=sharing)\n","\n","Alternatively, we can start implementing the A2C algorithm, using our probabilistic agent. We do so in [this notebook](https://colab.research.google.com/drive/1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x?usp=sharing)"],"metadata":{"id":"-AyAh795CJfc"}}],"metadata":{"colab":{"collapsed_sections":[],"name":"Building RL agents with AutoReset_corrige.ipynb","provenance":[{"file_id":"1VJUoDGhxKv3mmFjTmLj_JDpappVw29xh","timestamp":1655024396537},{"file_id":"1Ui481r47fNHCQsQfKwdoNEVrEiqAEokh","timestamp":1654866856386},{"file_id":"1yAQlrShysj4Q9EBpYM8pBsp2aXInhP7x","timestamp":1650560438416},{"file_id":"1XUJSplQm_MttDKtzsJ1AKhhJQT_W0oWi","timestamp":1650557706114},{"file_id":"1J74foctf26QfZ4DuKxGfAwrO2wZmedNi","timestamp":1643612886194},{"file_id":"1-aidxjij0JwVyOgYSLqR-v4KMow4BsbQ","timestamp":1641470409971},{"file_id":"1tZ744yXYoDhwk0xk73baYa7Ks4MRpba8","timestamp":1641465913520},{"file_id":"1SEFpe1yUMjUsKzYkqWF_xQzVzZOQutHZ","timestamp":1641289551712}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}