{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caebcf98",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "This notebook is designed to understand how to use a gymnasium environment as a BBRL agent in practice, in the autoreset=False mode.\n",
    "It is part of the [BBRL documentation](https://github.com/osigaud/bbrl/docs/index.html).\n",
    "\n",
    "If this is your first contact with BBRL, you may start be having a look at [this more basic notebook](01-basic_concepts.student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992864b",
   "metadata": {},
   "source": [
    "## Installation and Imports\n",
    "\n",
    "### Installation\n",
    "\n",
    "The BBRL library is [here](https://github.com/osigaud/bbrl).\n",
    "\n",
    "We use OmegaConf to that makes it possible that by just defining the `def\n",
    "run_dqn(cfg):` function and then executing a long `params = {...}` variable at\n",
    "the bottom of this colab, the code is run with the parameters without calling\n",
    "an explicit main.\n",
    "\n",
    "More precisely, the code is run by calling\n",
    "\n",
    "`config=OmegaConf.create(params)`\n",
    "\n",
    "`run_dqn(config)`\n",
    "\n",
    "at the very bottom of the colab, after starting tensorboard.\n",
    "\n",
    "Below, we import standard python packages, pytorch packages and gymnasium\n",
    "environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2d0403",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_gymnasium>=0.2.0\n",
      "[easypip] Installing bbrl_gymnasium[box2d]\n",
      "[easypip] Installing bbrl_gymnasium[classic_control]\n"
     ]
    }
   ],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install easypip\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"bbrl_gymnasium[box2d]\")\n",
    "easyinstall(\"bbrl_gymnasium[classic_control]\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd42023",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Optional\n",
    "from functools import partial\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import bbrl_gymnasium\n",
    "\n",
    "import copy\n",
    "from abc import abstractmethod, ABC\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import strftime\n",
    "OmegaConf.register_new_resolver(\n",
    "    \"current_time\", lambda: strftime(\"%Y%m%d-%H%M%S\"), replace=True\n",
    ")\n",
    "\n",
    "\n",
    "testing_mode = os.environ.get(\"TESTING_MODE\", None) == \"ON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49888eca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports all the necessary classes and functions from BBRL\n",
    "from bbrl.agents.agent import Agent\n",
    "from bbrl import get_arguments, get_class, instantiate_class\n",
    "# The workspace is the main class in BBRL, this is where all data is collected and stored\n",
    "from bbrl.workspace import Workspace\n",
    "\n",
    "# Agents(agent1,agent2,agent3,...) executes the different agents the one after the other\n",
    "# TemporalAgent(agent) executes an agent over multiple timesteps in the workspace, \n",
    "# or until a given condition is reached\n",
    "from bbrl.agents import Agents, TemporalAgent\n",
    "\n",
    "# ParallelGymAgent is an agent able to execute a batch of gymnasium environments\n",
    "# with auto-resetting. These agents produce multiple variables in the workspace:\n",
    "# ’env/env_obs’, ’env/reward’, ’env/timestep’, ’env/terminated’,\n",
    "# 'env/truncated', 'env/done', ’env/cumulated_reward’, ... \n",
    "# \n",
    "# When called at timestep t=0, the environments are automatically reset. At\n",
    "# timestep t>0, these agents will read the ’action’ variable in the workspace at\n",
    "# time t − 1\n",
    "from bbrl.agents.gymnasium import GymAgent, ParallelGymAgent, make_env, record_video\n",
    "\n",
    "# Replay buffers are useful to store past transitions when training\n",
    "from bbrl.utils.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7b839b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 1,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Utility function for launching tensorboard\n",
    "# For Colab - otherwise, it is easier and better to launch tensorboard from\n",
    "# the terminal\n",
    "def setup_tensorboard(path):\n",
    "    path = Path(path)\n",
    "    answer = \"\"\n",
    "    if is_notebook():\n",
    "        if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "            answer = \"y\"\n",
    "        while answer not in [\"y\", \"n\"]:\n",
    "                answer = input(f\"Do you want to launch tensorboard in this notebook [y/n] \").lower()\n",
    "\n",
    "    if answer == \"y\":\n",
    "        get_ipython().run_line_magic(\"load_ext\", \"tensorboard\")\n",
    "        get_ipython().run_line_magic(\"tensorboard\", f\"--logdir {path.absolute()}\")\n",
    "    else:\n",
    "        import sys\n",
    "        import os\n",
    "        import os.path as osp\n",
    "        print(f\"Launch tensorboard from the shell:\\n{osp.dirname(sys.executable)}/tensorboard --logdir={path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f776fa0d",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [],
   "source": [
    "if not is_notebook():\n",
    "    print(\"Not displaying video (hidden since not in a notebook)\", file=sys.stderr)\n",
    "    def video_display(*args, **kwargs):\n",
    "        pass\n",
    "    def display(*args, **kwargs):\n",
    "        print(*args, **kwargs) \n",
    "    \n",
    "testing_mode = os.environ.get(\"TESTING_MODE\", None) == \"ON\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f8df1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "We first create an Agent representing [the CartPole-v1 gym environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "This is done using the [ParallelGymAgent](https://github.com/osigaud/bbrl/blob/40fe0468feb8998e62c3cd6bb3a575fef88e256f/src/bbrl/agents/gymnasium.py#L261) class.\n",
    "We are working with batches (i.e. several episodes at the same time), so here our Agent uses n_envs = 3 environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092489b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: observation space in R^4 and action space {1, ..., 2}\n"
     ]
    }
   ],
   "source": [
    "# We run episodes over 3 environments at a time\n",
    "n_envs = 3\n",
    "env_agent = ParallelGymAgent(partial(make_env, 'CartPole-v1', autoreset=False), n_envs, reward_at_t=False)\n",
    "# The random seed is set to 2139\n",
    "env_agent.seed(2139)\n",
    "\n",
    "obs_size, action_dim = env_agent.get_obs_and_actions_sizes()\n",
    "print(f\"Environment: observation space in R^{obs_size} and action space {{1, ..., {action_dim}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421b65ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation tensor([[-0.0085, -0.0427, -0.0489,  0.0215],\n",
      "        [ 0.0005,  0.0025, -0.0493, -0.0402],\n",
      "        [ 0.0080,  0.0203, -0.0023, -0.0085]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a new workspace\n",
    "workspace = Workspace() \n",
    "\n",
    "# Execute the first step\n",
    "env_agent(workspace, t=0)\n",
    "\n",
    "# Our first set of observations. The size of the observation space is 4, and we have 3 environments.\n",
    "obs = workspace.get(\"env/env_obs\", 0)\n",
    "print(\"Observation\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654dc868",
   "metadata": {},
   "source": [
    "### Random action without agent\n",
    "We first set an action directly without using an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a2147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.1531, -0.0485, -0.2862],\n",
       "        [ 0.0006,  0.1983, -0.0501, -0.3480],\n",
       "        [ 0.0084, -0.1747, -0.0025,  0.2834]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sets the next action\n",
    "action = torch.randint(0, action_dim, (n_envs, ))\n",
    "workspace.set(\"action\", 0, action)\n",
    "print(action)\n",
    "env_agent(workspace, t=1)\n",
    "\n",
    "# And perform one step\n",
    "workspace.get(\"env/env_obs\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab775897",
   "metadata": {},
   "source": [
    "Let us now see the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2334b9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env/env_obs tensor([[[-0.0085, -0.0427, -0.0489,  0.0215],\n",
      "         [ 0.0005,  0.0025, -0.0493, -0.0402],\n",
      "         [ 0.0080,  0.0203, -0.0023, -0.0085]],\n",
      "\n",
      "        [[-0.0094,  0.1531, -0.0485, -0.2862],\n",
      "         [ 0.0006,  0.1983, -0.0501, -0.3480],\n",
      "         [ 0.0084, -0.1747, -0.0025,  0.2834]]])\n",
      "env/terminated tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/truncated tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/done tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "env/reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "env/cumulated_reward tensor([[0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "env/timestep tensor([[0, 0, 0],\n",
      "        [1, 1, 1]])\n",
      "action tensor([[1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "for key in workspace.variables.keys():\n",
    "    print(key, workspace[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d85200",
   "metadata": {},
   "source": [
    "You can observe that we have two time steps for each variable that are stored\n",
    "within tensors where the first dimension is time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94da3b",
   "metadata": {},
   "source": [
    "### Random agent\n",
    "\n",
    "The process above can be\n",
    "automatized with `Agents` and `TemporalAgent` as shown below - but first we have\n",
    "to create an agent that selects the actions (here, random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15533ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def forward(self, t: int, choose_action=True, **kwargs):\n",
    "        \"\"\"An Agent can use self.workspace\"\"\"\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = torch.randint(0, self.action_dim, (len(obs), ))\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "# Each agent is run in the order given when constructing Agents\n",
    "agents = Agents(env_agent, RandomAgent(action_dim))\n",
    "\n",
    "# And the TemporalAgent allows to run through time\n",
    "t_agents = TemporalAgent(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f6df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now run the agents throught time with a simple call...\n",
    "\n",
    "workspace = Workspace()\n",
    "t_agents(workspace, t=0, stop_variable=\"env/done\", stochastic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14927679",
   "metadata": {},
   "source": [
    "### Termination\n",
    "\n",
    "`env/done` tells us if the episode was finished or not\n",
    "here, with NoAutoReset, (1) we wait that all episodes are \"done\",\n",
    "and when an episode is finished the flag remains True.\n",
    "Note that when an environment is done before the others, its content is copied until the termination of all environments.\n",
    "This is convenient for collecting the final reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c097a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([38, 3]),\n",
       " tensor([[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True, False,  True],\n",
       "         [ True,  True,  True]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/done\"].shape, workspace[\"env/done\"][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca559b",
   "metadata": {},
   "source": [
    "The resulting tensor of observations, with the last two observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "821f4e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([38, 3, 4]),\n",
       " tensor([[[ 0.1225, -0.1720, -0.2107, -0.4356],\n",
       "          [-0.0660, -0.8039,  0.2033,  1.6900],\n",
       "          [-0.0261, -0.7726,  0.2220,  1.4842]],\n",
       " \n",
       "         [[ 0.1225, -0.1720, -0.2107, -0.4356],\n",
       "          [-0.0821, -0.6117,  0.2371,  1.4669],\n",
       "          [-0.0261, -0.7726,  0.2220,  1.4842]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/env_obs\"].shape, workspace[\"env/env_obs\"][-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b316ef",
   "metadata": {},
   "source": [
    "The resulting tensor of rewards, with the last 8 rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5192627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([38, 3]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 1., 0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"env/reward\"].shape, workspace[\"env/reward\"][-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c19645",
   "metadata": {},
   "source": [
    "The resulting tensor of actions, with the last two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "289ee964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([38, 3]),\n",
       " tensor([[1, 1, 0],\n",
       "         [1, 1, 1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace[\"action\"].shape, workspace[\"action\"][-2:]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
